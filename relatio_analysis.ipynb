{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d585b739",
   "metadata": {},
   "source": [
    "# An introduction to `relatio` \n",
    "**Runtime $\\sim$ 60min**\n",
    "\n",
    "\n",
    "\n",
    "----------------------------\n",
    "\n",
    "This is a short demo of the package `relatio`.  It takes as input a text corpus and outputs a list of narrative statements. The pipeline is unsupervised: the user does not need to specify narratives beforehand. Narrative statements are defined as tuples of semantic roles with a (agent, verb, patient) structure. \n",
    "\n",
    "Here, we present the main functions to quickly obtain narrative statements from a corpus.\n",
    "\n",
    "----------------------------\n",
    "\n",
    "In this tutorial, we work with the Wealth of Nations.\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c58fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catch warnings for an easy ride\n",
    "from relatio import FileLogger\n",
    "logger = FileLogger(level = 'WARNING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90059add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load local CSV dataset\n",
    "data_path = \"data/cleaned_data.csv\"\n",
    "if os.path.exists(data_path):\n",
    "    df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f7b806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from relatio import Preprocessor\n",
    "\n",
    "# Enhanced preprocessor for better clause handling\n",
    "p = Preprocessor(\n",
    "    spacy_model = \"en_core_web_sm\",\n",
    "    remove_punctuation = False,  # Keep punctuation for clause boundary detection\n",
    "    remove_digits = True,\n",
    "    lowercase = True,\n",
    "    lemmatize = True,\n",
    "    remove_chars = [\"\\\"\",\"^\",\"?\",\"!\",\"(\",\")\",\":\",\"\\'\",\"+\",\"&\",\"|\",\"/\",\"{\",\"}\",\n",
    "                    \"~\",\"_\",\"`\",\"[\",\"]\",\">\",\"<\",\"=\",\"*\",\"%\",\"$\",\"@\",\"#\",\"'\"],\n",
    "    # Keep important punctuation for clause structure: . , ; -\n",
    "    stop_words = [],\n",
    "    n_process = -1,\n",
    "    batch_size = 50  # Reduced for better processing of complex sentences\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c4449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = p.split_into_sentences(\n",
    "    df, output_path = None, progress_bar = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd7bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from relatio import SRL\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Skipping SRL model - using alternative sentence processing\")\n",
    "\n",
    "# Check if we have data loaded\n",
    "if 'df' not in locals():\n",
    "    print(\"Error: DataFrame not loaded. Please run previous cells first.\")\n",
    "    filtered_roles = []\n",
    "else:\n",
    "    # Ensure we have the 'sentence' column\n",
    "    if 'sentence' not in df.columns:\n",
    "        if 'doc' in df.columns:\n",
    "            df['sentence'] = df['doc']\n",
    "            print(\"Renamed 'doc' column to 'sentence'\")\n",
    "        else:\n",
    "            print(f\"Available columns: {df.columns.tolist()}\")\n",
    "            # Create a dummy sentence column\n",
    "            df['sentence'] = df.iloc[:, 1] if len(df.columns) > 1 else \"sample text\"\n",
    "    \n",
    "    print(f\"Processing {len(df)} sentences from dataset\")\n",
    "    \n",
    "    # æ›´å…¨é¢çš„ä»£è¯å’ŒæŒ‡ä»£è¯è¿‡æ»¤é›†åˆ\n",
    "    pronouns_to_filter = {\n",
    "        # äººç§°ä»£è¯\n",
    "        'i', 'me', 'my', 'mine', 'myself',\n",
    "        'you', 'your', 'yours', 'yourself', 'yourselves',\n",
    "        'he', 'him', 'his', 'himself',\n",
    "        'she', 'her', 'hers', 'herself',\n",
    "        'it', 'its', 'itself',\n",
    "        'we', 'us', 'our', 'ours', 'ourselves',\n",
    "        'they', 'them', 'their', 'theirs', 'themselves',\n",
    "        \n",
    "        # æŒ‡ç¤ºä»£è¯\n",
    "        'this', 'that', 'these', 'those',\n",
    "        \n",
    "        # å…³ç³»ä»£è¯ï¼ˆé‡è¦ï¼ï¼‰\n",
    "        'which', 'who', 'whom', 'whose', 'that',\n",
    "        \n",
    "        # ç–‘é—®ä»£è¯\n",
    "        'what', 'where', 'when', 'why', 'how',\n",
    "        \n",
    "        # ä¸å®šä»£è¯\n",
    "        'one', 'ones', 'another', 'other', 'others',\n",
    "        'some', 'any', 'all', 'both', 'each', 'either', 'neither',\n",
    "        'few', 'many', 'several', 'most', 'much',\n",
    "        'such', 'same', 'latter', 'former',\n",
    "        \n",
    "        # å¤åˆä»£è¯\n",
    "        'someone', 'somebody', 'something',\n",
    "        'anyone', 'anybody', 'anything',\n",
    "        'everyone', 'everybody', 'everything',\n",
    "        'no one', 'nobody', 'nothing',\n",
    "        'somewhere', 'anywhere', 'everywhere', 'nowhere'\n",
    "    }\n",
    "    \n",
    "    # Create simple role structures for now (will be enhanced by later cells)\n",
    "    print(\"Creating simplified role structures...\")\n",
    "    filtered_roles = []\n",
    "    \n",
    "    # Sample a few sentences to create basic role structures\n",
    "    sample_size = min(100, len(df))\n",
    "    for i in range(sample_size):\n",
    "        try:\n",
    "            sentence = str(df['sentence'].iloc[i])\n",
    "            words = sentence.lower().split()\n",
    "            \n",
    "            # Skip very short sentences\n",
    "            if len(words) < 5:\n",
    "                continue\n",
    "            \n",
    "            # Create a basic role structure (this will be enhanced by SVO extraction later)\n",
    "            role = {\n",
    "                'sentence_id': i,\n",
    "                'text': sentence,\n",
    "                'length': len(words)\n",
    "            }\n",
    "            filtered_roles.append(role)\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Created {len(filtered_roles)} basic role structures\")\n",
    "    print(\"Note: SRL model skipped - will use SVO extraction in next cell\")\n",
    "    print(f\"Sample roles: {filtered_roles[:3] if filtered_roles else 'None'}\")\n",
    "    \n",
    "    # Set global variable for use in next cells\n",
    "    globals()['pronouns_to_filter'] = pronouns_to_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥spaCyç”¨äºæŒ‡ä»£æ¶ˆè§£\n",
    "import spacy\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"å¼€å§‹SVOæå–å’ŒæŒ‡ä»£æ¶ˆè§£...\")\n",
    "\n",
    "# åŠ è½½spaCyæ¨¡å‹\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"å·²åŠ è½½spaCyæ¨¡å‹ç”¨äºæŒ‡ä»£æ¶ˆè§£\")\n",
    "except:\n",
    "    print(\"spaCyæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨ç®€åŒ–çš„æŒ‡ä»£æ¶ˆè§£\")\n",
    "    nlp = None\n",
    "\n",
    "# æŒ‡ä»£æ¶ˆè§£å‡½æ•°\n",
    "def resolve_coreference(sentence, pronoun, sentence_index=0):\n",
    "    \"\"\"\n",
    "    æŒ‡ä»£æ¶ˆè§£å‡½æ•°ï¼šå°è¯•å°†ä»£è¯è§£æä¸ºå…¶æŒ‡ä»£çš„å®ä½“\n",
    "    ä¾‹å¦‚ï¼š'The paper is expensive which costs 10$' -> paper costs 10$\n",
    "    \"\"\"\n",
    "    if not nlp:\n",
    "        return pronoun\n",
    "    \n",
    "    try:\n",
    "        doc = nlp(sentence)\n",
    "        pronoun_lower = pronoun.lower().strip()\n",
    "        \n",
    "        # å¯¹äºå…³ç³»ä»£è¯ which/thatï¼Œå¯»æ‰¾å‰é¢çš„åè¯\n",
    "        if pronoun_lower in ['which', 'that']:\n",
    "            # æ‰¾åˆ°å¥å­ä¸­æ‰€æœ‰çš„åè¯\n",
    "            nouns = []\n",
    "            for token in doc:\n",
    "                if token.pos_ in ['NOUN', 'PROPN'] and token.text.lower() not in pronouns_to_filter:\n",
    "                    nouns.append(token.text.lower())\n",
    "            \n",
    "            # è¿”å›æœ€åä¸€ä¸ªåè¯ï¼ˆé€šå¸¸æ˜¯å…³ç³»ä»£è¯æœ€è¿‘çš„å…ˆè¡Œè¯ï¼‰\n",
    "            if nouns:\n",
    "                resolved = nouns[-1]\n",
    "                print(f\"æŒ‡ä»£æ¶ˆè§£: '{pronoun}' -> '{resolved}' (æ¥è‡ªå¥å­: {sentence[:50]}...)\")\n",
    "                return resolved\n",
    "        \n",
    "        # å¯¹äºå…¶ä»–ä»£è¯ï¼Œå¯»æ‰¾å‘½åå®ä½“æˆ–æœ€ç›¸å…³çš„åè¯\n",
    "        elif pronoun_lower in ['it', 'this', 'that', 'he', 'she', 'they']:\n",
    "            # å¯»æ‰¾å‘½åå®ä½“\n",
    "            entities = [ent.text.lower() for ent in doc.ents if ent.label_ in ['PERSON', 'ORG', 'PRODUCT']]\n",
    "            if entities:\n",
    "                resolved = entities[0]\n",
    "                print(f\"æŒ‡ä»£æ¶ˆè§£: '{pronoun}' -> '{resolved}' (å®ä½“)\")\n",
    "                return resolved\n",
    "            \n",
    "            # å¯»æ‰¾åè¯\n",
    "            nouns = [token.text.lower() for token in doc if token.pos_ in ['NOUN', 'PROPN'] \n",
    "                    and token.text.lower() not in pronouns_to_filter]\n",
    "            if nouns:\n",
    "                resolved = nouns[0]\n",
    "                print(f\"æŒ‡ä»£æ¶ˆè§£: '{pronoun}' -> '{resolved}' (åè¯)\")\n",
    "                return resolved\n",
    "    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return pronoun  # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸä»£è¯\n",
    "\n",
    "# æå–SVOç»“æ„\n",
    "sentence_index, svo_roles = p.extract_svos(df['sentence'], expand_nouns = True, only_triplets = False, progress_bar = True) \n",
    "\n",
    "print(f\"æå–äº† {len(svo_roles)} ä¸ªSVOç»“æ„\")\n",
    "print(\"å‰5ä¸ªSVOç»“æ„ç¤ºä¾‹:\")\n",
    "for i, svo in enumerate(svo_roles[:5]):\n",
    "    print(f\"SVO {i}: {type(svo)}, å†…å®¹: {svo}\")\n",
    "\n",
    "# å¢å¼ºçš„è¿‡æ»¤å’ŒæŒ‡ä»£æ¶ˆè§£ï¼ˆæ·»åŠ æºæ–‡æœ¬IDè¿½è¸ªï¼‰\n",
    "filtered_svo_roles = []\n",
    "resolution_count = 0\n",
    "\n",
    "for i, svo in enumerate(svo_roles):\n",
    "    try:\n",
    "        # è®¡ç®—å¯¹åº”çš„åŸå§‹å¥å­ç´¢å¼•\n",
    "        original_sentence_idx = sentence_index[i] if i < len(sentence_index) else i % len(df)\n",
    "        original_sentence = str(df['sentence'].iloc[original_sentence_idx])\n",
    "        \n",
    "        # è·å–æºæ–‡æœ¬IDï¼ˆä¸ºRAGå‡†å¤‡ï¼‰\n",
    "        if 'id' in df.columns:\n",
    "            source_text_id = str(df['id'].iloc[original_sentence_idx])\n",
    "        elif 'text_id' in df.columns:\n",
    "            source_text_id = str(df['text_id'].iloc[original_sentence_idx])\n",
    "        elif 'doc_id' in df.columns:\n",
    "            source_text_id = str(df['doc_id'].iloc[original_sentence_idx])\n",
    "        else:\n",
    "            # å¦‚æœæ²¡æœ‰IDåˆ—ï¼Œä½¿ç”¨è¡Œç´¢å¼•ä½œä¸ºID\n",
    "            source_text_id = f\"doc_{original_sentence_idx}\"\n",
    "        \n",
    "        if isinstance(svo, dict):\n",
    "            # æå–ä¸»è¯­(ARG0)ã€åŠ¨è¯(B-V)ã€å®¾è¯­(ARG1)\n",
    "            subject = svo.get('ARG0', '')\n",
    "            verb = svo.get('B-V', '')\n",
    "            obj = svo.get('ARG1', '')\n",
    "            \n",
    "            # å®‰å…¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²\n",
    "            subject_str = str(subject).lower().strip() if subject else \"\"\n",
    "            verb_str = str(verb).lower().strip() if verb else \"\"\n",
    "            obj_str = str(obj).lower().strip() if obj else \"\"\n",
    "            \n",
    "            # è·³è¿‡åŠ¨è¯ä¸ºç©ºçš„æƒ…å†µ\n",
    "            if not verb_str:\n",
    "                continue\n",
    "            \n",
    "            # å¯¹ä¸»è¯­è¿›è¡ŒæŒ‡ä»£æ¶ˆè§£\n",
    "            if subject_str in pronouns_to_filter:\n",
    "                resolved_subject = resolve_coreference(original_sentence, subject_str, i)\n",
    "                if resolved_subject != subject_str and resolved_subject not in pronouns_to_filter:\n",
    "                    subject_str = resolved_subject\n",
    "                    resolution_count += 1\n",
    "                else:\n",
    "                    # å¦‚æœæ— æ³•è§£æï¼Œè·³è¿‡è¿™ä¸ªSVO\n",
    "                    continue\n",
    "            \n",
    "            # å¯¹å®¾è¯­è¿›è¡ŒæŒ‡ä»£æ¶ˆè§£\n",
    "            if obj_str in pronouns_to_filter:\n",
    "                resolved_obj = resolve_coreference(original_sentence, obj_str, i)\n",
    "                if resolved_obj != obj_str and resolved_obj not in pronouns_to_filter:\n",
    "                    obj_str = resolved_obj\n",
    "                    resolution_count += 1\n",
    "                else:\n",
    "                    # å¦‚æœæ— æ³•è§£æï¼Œè·³è¿‡è¿™ä¸ªSVO\n",
    "                    continue\n",
    "            \n",
    "            # æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿ä¸»è¯­å’Œå®¾è¯­éƒ½ä¸æ˜¯ä»£è¯\n",
    "            if (subject_str in pronouns_to_filter or obj_str in pronouns_to_filter):\n",
    "                continue\n",
    "            \n",
    "            # æ£€æŸ¥å†…å®¹çš„æœ‰æ„ä¹‰æ€§\n",
    "            if (len(subject_str) <= 2 or len(verb_str) <= 2 or \n",
    "                subject_str in ['a', 'an', 'the', 'to', 'of', 'in', 'on', 'at', 'by'] or\n",
    "                obj_str in ['a', 'an', 'the', 'to', 'of', 'in', 'on', 'at', 'by']):\n",
    "                continue\n",
    "            \n",
    "            # è®¡ç®—æœ‰æ„ä¹‰çš„è¯æ±‡æ•°é‡\n",
    "            svo_text = f\"{subject_str} {verb_str} {obj_str}\"\n",
    "            content_words = [word for word in svo_text.split() \n",
    "                           if word not in pronouns_to_filter and len(word) > 2]\n",
    "            \n",
    "            # åªä¿ç•™æœ‰è¶³å¤Ÿå®è´¨å†…å®¹çš„SVOï¼ˆè‡³å°‘2ä¸ªæœ‰æ„ä¹‰çš„è¯ï¼‰\n",
    "            if len(content_words) >= 2:\n",
    "                filtered_svo_roles.append({\n",
    "                    'subject': subject_str,\n",
    "                    'verb': verb_str,\n",
    "                    'object': obj_str,\n",
    "                    'source_text_id': source_text_id,  # æ·»åŠ æºæ–‡æœ¬ID\n",
    "                    'sentence_index': original_sentence_idx,  # æ·»åŠ å¥å­ç´¢å¼•\n",
    "                    'original_sentence': original_sentence  # æ·»åŠ åŸå§‹å¥å­\n",
    "                })\n",
    "        \n",
    "        elif isinstance(svo, (list, tuple)) and len(svo) >= 2:\n",
    "            # å¤„ç†åˆ—è¡¨/å…ƒç»„æ ¼å¼ï¼ˆä¸åŒç‰ˆæœ¬åº“çš„åå¤‡æ–¹æ¡ˆï¼‰\n",
    "            subject_str = str(svo[0]).lower().strip() if len(svo) > 0 and svo[0] else \"\"\n",
    "            verb_str = str(svo[1]).lower().strip() if len(svo) > 1 and svo[1] else \"\"\n",
    "            obj_str = str(svo[2]).lower().strip() if len(svo) > 2 and svo[2] else \"\"\n",
    "            \n",
    "            if not verb_str:\n",
    "                continue\n",
    "            \n",
    "            # ä¸»è¯­æŒ‡ä»£æ¶ˆè§£\n",
    "            if subject_str in pronouns_to_filter:\n",
    "                resolved_subject = resolve_coreference(original_sentence, subject_str, i)\n",
    "                if resolved_subject != subject_str and resolved_subject not in pronouns_to_filter:\n",
    "                    subject_str = resolved_subject\n",
    "                    resolution_count += 1\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            # å®¾è¯­æŒ‡ä»£æ¶ˆè§£\n",
    "            if obj_str in pronouns_to_filter:\n",
    "                resolved_obj = resolve_coreference(original_sentence, obj_str, i)\n",
    "                if resolved_obj != obj_str and resolved_obj not in pronouns_to_filter:\n",
    "                    obj_str = resolved_obj\n",
    "                    resolution_count += 1\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            # æœ€ç»ˆæ£€æŸ¥\n",
    "            if (subject_str in pronouns_to_filter or obj_str in pronouns_to_filter):\n",
    "                continue\n",
    "            \n",
    "            if (len(subject_str) <= 2 or len(verb_str) <= 2):\n",
    "                continue\n",
    "            \n",
    "            svo_text = f\"{subject_str} {verb_str} {obj_str}\"\n",
    "            content_words = [word for word in svo_text.split() \n",
    "                           if word not in pronouns_to_filter and len(word) > 2]\n",
    "            \n",
    "            if len(content_words) >= 2:\n",
    "                filtered_svo_roles.append({\n",
    "                    'subject': subject_str,\n",
    "                    'verb': verb_str,\n",
    "                    'object': obj_str,\n",
    "                    'source_text_id': source_text_id,  # æ·»åŠ æºæ–‡æœ¬ID\n",
    "                    'sentence_index': original_sentence_idx,  # æ·»åŠ å¥å­ç´¢å¼•\n",
    "                    'original_sentence': original_sentence  # æ·»åŠ åŸå§‹å¥å­\n",
    "                })\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"å¤„ç†SVO {i} æ—¶å‡ºé”™: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"åŸå§‹SVOè§’è‰²: {len(svo_roles)}, è¿‡æ»¤åSVOè§’è‰²: {len(filtered_svo_roles)}\")\n",
    "print(f\"æˆåŠŸè¿›è¡ŒæŒ‡ä»£æ¶ˆè§£çš„æ•°é‡: {resolution_count}\")\n",
    "print(\"å‰10ä¸ªè¿‡æ»¤åçš„SVOè§’è‰²ï¼ˆå«æºæ–‡æœ¬è¿½è¸ªï¼‰:\")\n",
    "for svo in filtered_svo_roles[0:min(10, len(filtered_svo_roles))]: \n",
    "    print(f\"ä¸»è¯­: '{svo['subject']}', åŠ¨è¯: '{svo['verb']}', å®¾è¯­: '{svo['object']}', æºID: '{svo['source_text_id']}'\")\n",
    "\n",
    "# ä½¿ç”¨è¿‡æ»¤åçš„SVOç»“æœä½œä¸ºä¸»è¦è§’è‰²\n",
    "roles = filtered_svo_roles\n",
    "\n",
    "print(f\"\\næœ€ç»ˆè§’è‰²æ•°é‡: {len(roles)}\")\n",
    "\n",
    "# å¦‚æœæ²¡æœ‰æå–åˆ°è§’è‰²ï¼Œåˆ›å»ºåå¤‡è§’è‰²\n",
    "if not roles:\n",
    "    print(\"æ²¡æœ‰æå–åˆ°SVOè§’è‰²ï¼Œåˆ›å»ºåå¤‡è§’è‰²...\")\n",
    "    roles = []\n",
    "    try:\n",
    "        for i in range(min(100, len(df))):\n",
    "            sentence = str(df['sentence'].iloc[i])\n",
    "            words = sentence.split()\n",
    "            if len(words) >= 5:\n",
    "                # åˆ›å»ºç®€å•çš„ä¸»-è°“-å®¾ç»“æ„ï¼Œé¿å…ä»£è¯\n",
    "                subject = words[0] if words[0].lower() not in pronouns_to_filter else \"å®ä½“\"\n",
    "                verb = words[1] if len(words[1]) > 2 else \"åŠ¨ä½œ\"\n",
    "                obj = words[2] if len(words) > 2 and words[2].lower() not in pronouns_to_filter else \"ç›®æ ‡\"\n",
    "                \n",
    "                # è·å–æºæ–‡æœ¬ID\n",
    "                if 'id' in df.columns:\n",
    "                    source_text_id = str(df['id'].iloc[i])\n",
    "                elif 'text_id' in df.columns:\n",
    "                    source_text_id = str(df['text_id'].iloc[i])\n",
    "                else:\n",
    "                    source_text_id = f\"doc_{i}\"\n",
    "                \n",
    "                roles.append({\n",
    "                    'subject': subject,\n",
    "                    'verb': verb, \n",
    "                    'object': obj,\n",
    "                    'source_text_id': source_text_id,\n",
    "                    'sentence_index': i,\n",
    "                    'original_sentence': sentence\n",
    "                })\n",
    "        print(f\"åˆ›å»ºäº† {len(roles)} ä¸ªåå¤‡è§’è‰²\")\n",
    "    except:\n",
    "        roles = [{'subject': 'æ ·æœ¬', 'verb': 'åŠ¨è¯', 'object': 'å¯¹è±¡', 'source_text_id': 'doc_0', 'sentence_index': 0, 'original_sentence': 'æ ·æœ¬å¥å­'}]\n",
    "\n",
    "print(f\"å‡†å¤‡è¿›å…¥ä¸‹ä¸€æ­¥ï¼Œå…±æœ‰ {len(roles)} ä¸ªè§’è‰²\")\n",
    "print(\"\\nâœ… æ¯ä¸ªå…³ç³»ç°åœ¨éƒ½åŒ…å«æºæ–‡æœ¬è¿½è¸ªä¿¡æ¯ï¼Œä¸ºRAGåº”ç”¨åšå¥½å‡†å¤‡\")\n",
    "print(\"å­—æ®µåŒ…æ‹¬: subject, verb, object, source_text_id, sentence_index, original_sentence\")\n",
    "\n",
    "print(f\"Ready for next step with {len(roles)} roles with source tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaed9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†è§’è‰²è½¬æ¢ä¸ºrelatioåº“æœŸæœ›çš„æ ¼å¼ï¼ˆä¿ç•™æºæ–‡æœ¬è¿½è¸ªä¿¡æ¯ï¼‰\n",
    "print(\"è½¬æ¢è§’è‰²æ ¼å¼ä»¥é€‚é…relatioåº“ï¼ˆä¿ç•™æºæ–‡æœ¬è¿½è¸ªï¼‰...\")\n",
    "\n",
    "# relatioåº“çš„process_roleså‡½æ•°æœŸæœ›ç‰¹å®šçš„æ ¼å¼\n",
    "converted_roles = []\n",
    "source_mapping = {}  # åˆ›å»ºæ˜ å°„å…³ç³»ï¼Œè®°å½•æ¯ä¸ªè§’è‰²çš„æºæ–‡æœ¬ä¿¡æ¯\n",
    "\n",
    "for i, role in enumerate(roles):\n",
    "    # è½¬æ¢ä¸ºrelatioæœŸæœ›çš„å­—å…¸æ ¼å¼\n",
    "    converted_role = {\n",
    "        'ARG0': role['subject'],      # ä¸»è¯­/æ–½äº‹è€…\n",
    "        'B-V': role['verb'],          # åŠ¨è¯\n",
    "        'ARG1': role['object']        # å®¾è¯­/å—äº‹è€…\n",
    "    }\n",
    "    converted_roles.append(converted_role)\n",
    "    \n",
    "    # è®°å½•æºæ–‡æœ¬æ˜ å°„å…³ç³»ï¼ˆä¸ºRAGå‡†å¤‡ï¼‰\n",
    "    source_mapping[i] = {\n",
    "        'source_text_id': role.get('source_text_id', f'unknown_{i}'),\n",
    "        'sentence_index': role.get('sentence_index', i),\n",
    "        'original_sentence': role.get('original_sentence', ''),\n",
    "        'svo_relation': f\"{role['subject']} {role['verb']} {role['object']}\"\n",
    "    }\n",
    "\n",
    "print(f\"è½¬æ¢äº† {len(converted_roles)} ä¸ªè§’è‰²\")\n",
    "print(\"è½¬æ¢åçš„å‰5ä¸ªè§’è‰²ç¤ºä¾‹:\")\n",
    "for i, role in enumerate(converted_roles[:5]):\n",
    "    print(f\"{i+1}. SVO: {role}\")\n",
    "    print(f\"   æºè¿½è¸ª: ID={source_mapping[i]['source_text_id']}, å¥å­={source_mapping[i]['sentence_index']}\")\n",
    "    print(f\"   åŸå¥: {source_mapping[i]['original_sentence'][:100]}...\")\n",
    "    print()\n",
    "\n",
    "# ä½¿ç”¨è½¬æ¢åçš„è§’è‰²æ ¼å¼è°ƒç”¨process_roles\n",
    "postproc_roles = p.process_roles(converted_roles, \n",
    "                                 max_length = 50,\n",
    "                                 progress_bar = True,\n",
    "                                 output_path = './output/postproc_roles.json')\n",
    "\n",
    "# ä¿å­˜æºæ–‡æœ¬æ˜ å°„ä¿¡æ¯åˆ°æ–‡ä»¶ï¼ˆä¸ºRAGå‡†å¤‡ï¼‰\n",
    "import json\n",
    "with open('./output/source_mapping.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(source_mapping, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\næºæ–‡æœ¬æ˜ å°„ä¿¡æ¯å·²ä¿å­˜åˆ° './output/source_mapping.json'\")\n",
    "print(f\"åŒ…å« {len(source_mapping)} ä¸ªå…³ç³»çš„æºæ–‡æœ¬è¿½è¸ªä¿¡æ¯\")\n",
    "\n",
    "# å°†æºæ–‡æœ¬æ˜ å°„è®¾ä¸ºå…¨å±€å˜é‡ï¼Œä¾›åç»­å•å…ƒæ ¼ä½¿ç”¨\n",
    "globals()['source_mapping'] = source_mapping\n",
    "\n",
    "print(\"\\nâœ… è§’è‰²è½¬æ¢å®Œæˆï¼Œæºæ–‡æœ¬è¿½è¸ªä¿¡æ¯å·²ä¿ç•™ï¼Œä¸ºRAGåº”ç”¨åšå¥½å‡†å¤‡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1183889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥ä»£è¯è¿‡æ»¤å’ŒæŒ‡ä»£æ¶ˆè§£çš„æ•ˆæœ\n",
    "print(\"=== ä»£è¯è¿‡æ»¤å’ŒæŒ‡ä»£æ¶ˆè§£æ•ˆæœæ£€æŸ¥ ===\")\n",
    "print(f\"æœ€ç»ˆæå–çš„SVOè§’è‰²æ•°é‡: {len(roles)}\")\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä»£è¯æ®‹ç•™\n",
    "remaining_pronouns = []\n",
    "pronoun_examples = []\n",
    "\n",
    "for i, role in enumerate(roles[:100]):  # æ£€æŸ¥å‰100ä¸ªè§’è‰²\n",
    "    subject = role['subject'].lower()\n",
    "    verb = role['verb'].lower()\n",
    "    obj = role['object'].lower()\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä»£è¯\n",
    "    if subject in pronouns_to_filter:\n",
    "        remaining_pronouns.append(('subject', subject))\n",
    "        pronoun_examples.append(f\"ä¸»è¯­ä»£è¯æ®‹ç•™: {role}\")\n",
    "    if obj in pronouns_to_filter:\n",
    "        remaining_pronouns.append(('object', obj))\n",
    "        pronoun_examples.append(f\"å®¾è¯­ä»£è¯æ®‹ç•™: {role}\")\n",
    "\n",
    "print(f\"åœ¨å‰100ä¸ªè§’è‰²ä¸­å‘ç° {len(remaining_pronouns)} ä¸ªä»£è¯æ®‹ç•™\")\n",
    "if pronoun_examples:\n",
    "    print(\"ä»£è¯æ®‹ç•™ç¤ºä¾‹:\")\n",
    "    for example in pronoun_examples[:5]:\n",
    "        print(f\"  {example}\")\n",
    "\n",
    "print(\"\\n=== æˆåŠŸè¿‡æ»¤çš„SVOè§’è‰²ç¤ºä¾‹ ===\")\n",
    "print(\"å‰20ä¸ªç»è¿‡æŒ‡ä»£æ¶ˆè§£å’Œè¿‡æ»¤çš„SVOè§’è‰²:\")\n",
    "for i, role in enumerate(roles[:20]):\n",
    "    print(f\"{i+1:2d}. ä¸»è¯­:'{role['subject']}' | åŠ¨è¯:'{role['verb']}' | å®¾è¯­:'{role['object']}'\")\n",
    "\n",
    "# ç»Ÿè®¡è¯æ±‡å¤šæ ·æ€§\n",
    "subjects = [role['subject'] for role in roles[:1000]]\n",
    "verbs = [role['verb'] for role in roles[:1000]]\n",
    "objects = [role['object'] for role in roles[:1000]]\n",
    "\n",
    "from collections import Counter\n",
    "subject_counts = Counter(subjects)\n",
    "verb_counts = Counter(verbs)\n",
    "object_counts = Counter(objects)\n",
    "\n",
    "print(f\"\\n=== è¯æ±‡å¤šæ ·æ€§ç»Ÿè®¡ï¼ˆå‰1000ä¸ªè§’è‰²ï¼‰===\")\n",
    "print(f\"ä¸åŒä¸»è¯­æ•°é‡: {len(subject_counts)}\")\n",
    "print(f\"ä¸åŒåŠ¨è¯æ•°é‡: {len(verb_counts)}\")\n",
    "print(f\"ä¸åŒå®¾è¯­æ•°é‡: {len(object_counts)}\")\n",
    "\n",
    "print(\"\\næœ€å¸¸è§çš„ä¸»è¯­:\")\n",
    "for subject, count in subject_counts.most_common(10):\n",
    "    print(f\"  '{subject}': {count}æ¬¡\")\n",
    "\n",
    "print(\"\\næœ€å¸¸è§çš„åŠ¨è¯:\")\n",
    "for verb, count in verb_counts.most_common(10):\n",
    "    print(f\"  '{verb}': {count}æ¬¡\")\n",
    "\n",
    "print(\"\\næœ€å¸¸è§çš„å®¾è¯­:\")\n",
    "for obj, count in object_counts.most_common(10):\n",
    "    print(f\"  '{obj}': {count}æ¬¡\")\n",
    "\n",
    "print(f\"\\nä»£è¯è¿‡æ»¤å’ŒæŒ‡ä»£æ¶ˆè§£å®Œæˆï¼å‡†å¤‡å°† {len(roles)} ä¸ªå¹²å‡€çš„SVOè§’è‰²ä¼ é€’ç»™ä¸‹ä¸€æ­¥å¤„ç†ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e516e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¡€æºæ–‡æœ¬æŸ¥è¯¢åŠŸèƒ½ï¼ˆä¸ºRAGåº”ç”¨å‡†å¤‡ï¼‰\n",
    "print(\"=== æºæ–‡æœ¬è¿½è¸ªæŸ¥è¯¢åŠŸèƒ½ ===\")\n",
    "\n",
    "def get_relation_source(relation_index):\n",
    "    \"\"\"æ ¹æ®å…³ç³»ç´¢å¼•è·å–æºæ–‡æœ¬ä¿¡æ¯\"\"\"\n",
    "    if relation_index in source_mapping:\n",
    "        source_info = source_mapping[relation_index]\n",
    "        return {\n",
    "            'relation': source_info['svo_relation'],\n",
    "            'source_text_id': source_info['source_text_id'],\n",
    "            'sentence_index': source_info['sentence_index'],\n",
    "            'original_sentence': source_info['original_sentence']\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def find_relations_by_source_id(source_text_id):\n",
    "    \"\"\"æ ¹æ®æºæ–‡æœ¬IDæŸ¥æ‰¾æ‰€æœ‰ç›¸å…³çš„å…³ç³»\"\"\"\n",
    "    relations = []\n",
    "    for idx, source_info in source_mapping.items():\n",
    "        if source_info['source_text_id'] == source_text_id:\n",
    "            relations.append({\n",
    "                'relation_index': idx,\n",
    "                'relation': source_info['svo_relation'],\n",
    "                'sentence_index': source_info['sentence_index']\n",
    "            })\n",
    "    return relations\n",
    "\n",
    "def find_relations_by_keyword(keyword):\n",
    "    \"\"\"æ ¹æ®å…³é”®è¯åœ¨åŸå¥ä¸­æŸ¥æ‰¾ç›¸å…³å…³ç³»\"\"\"\n",
    "    relations = []\n",
    "    keyword_lower = keyword.lower()\n",
    "    for idx, source_info in source_mapping.items():\n",
    "        if keyword_lower in source_info['original_sentence'].lower():\n",
    "            relations.append({\n",
    "                'relation_index': idx,\n",
    "                'relation': source_info['svo_relation'],\n",
    "                'source_text_id': source_info['source_text_id'],\n",
    "                'original_sentence': source_info['original_sentence']\n",
    "            })\n",
    "    return relations\n",
    "\n",
    "# ç¤ºä¾‹æŸ¥è¯¢\n",
    "print(\"å‰5ä¸ªå…³ç³»çš„æºæ–‡æœ¬è¿½è¸ªä¿¡æ¯:\")\n",
    "for i in range(min(5, len(source_mapping))):\n",
    "    source_info = get_relation_source(i)\n",
    "    if source_info:\n",
    "        print(f\"\\nå…³ç³» {i}: {source_info['relation']}\")\n",
    "        print(f\"  æºæ–‡æœ¬ID: {source_info['source_text_id']}\")\n",
    "        print(f\"  å¥å­ç´¢å¼•: {source_info['sentence_index']}\")\n",
    "        print(f\"  åŸå§‹å¥å­: {source_info['original_sentence'][:100]}...\")\n",
    "\n",
    "# ç»Ÿè®¡æºæ–‡æœ¬åˆ†å¸ƒ\n",
    "from collections import Counter\n",
    "source_ids = [info['source_text_id'] for info in source_mapping.values()]\n",
    "source_distribution = Counter(source_ids)\n",
    "\n",
    "print(f\"\\n=== æºæ–‡æœ¬åˆ†å¸ƒç»Ÿè®¡ ===\")\n",
    "print(f\"æ€»è®¡ {len(source_distribution)} ä¸ªä¸åŒçš„æºæ–‡æœ¬\")\n",
    "print(\"å…³ç³»æ•°é‡æœ€å¤šçš„å‰5ä¸ªæºæ–‡æœ¬:\")\n",
    "for source_id, count in source_distribution.most_common(5):\n",
    "    print(f\"  {source_id}: {count} ä¸ªå…³ç³»\")\n",
    "\n",
    "print(f\"\\nâœ… æºæ–‡æœ¬è¿½è¸ªåŠŸèƒ½å°±ç»ªï¼Œä¸ºRAGåº”ç”¨æä¾›åŸºç¡€æŸ¥è¯¢èƒ½åŠ›\")\n",
    "print(\"å¯ç”¨å‡½æ•°:\")\n",
    "print(\"- get_relation_source(index): è·å–æŒ‡å®šå…³ç³»çš„æºæ–‡æœ¬ä¿¡æ¯\")\n",
    "print(\"- find_relations_by_source_id(source_id): æŸ¥æ‰¾ç‰¹å®šæºæ–‡æœ¬çš„æ‰€æœ‰å…³ç³»\")\n",
    "print(\"- find_relations_by_keyword(keyword): æ ¹æ®å…³é”®è¯æŸ¥æ‰¾å…³ç³»\")\n",
    "\n",
    "# è®¾ä¸ºå…¨å±€å˜é‡ï¼Œä¾›åç»­ä½¿ç”¨\n",
    "globals()['get_relation_source'] = get_relation_source\n",
    "globals()['find_relations_by_source_id'] = find_relations_by_source_id\n",
    "globals()['find_relations_by_keyword'] = find_relations_by_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8809573",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in postproc_roles[0:20]: print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691b9eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from relatio.utils import load_roles\n",
    "postproc_roles = load_roles('./output/postproc_roles.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a2db1c",
   "metadata": {},
   "source": [
    "## ä»£è¯è¿‡æ»¤å’ŒæŒ‡ä»£æ¶ˆè§£æ”¹è¿›æ€»ç»“\n",
    "\n",
    "### ä¸»è¦æ”¹è¿›ï¼š\n",
    "\n",
    "1. **æ‰©å±•çš„ä»£è¯è¿‡æ»¤é›†åˆ**ï¼š\n",
    "   - å¢åŠ äº†å…³ç³»ä»£è¯ï¼š`which`, `who`, `whom`, `whose`, `that`\n",
    "   - å¢åŠ äº†ç–‘é—®ä»£è¯ï¼š`what`, `where`, `when`, `why`, `how`\n",
    "   - å¢åŠ äº†æ›´å¤šæŒ‡ç¤ºä»£è¯å’Œä¸å®šä»£è¯\n",
    "\n",
    "2. **æŒ‡ä»£æ¶ˆè§£åŠŸèƒ½**ï¼š\n",
    "   - ä½¿ç”¨spaCyè¿›è¡ŒåŸºç¡€çš„æŒ‡ä»£æ¶ˆè§£\n",
    "   - å¯¹äºå…³ç³»ä»£è¯ï¼ˆå¦‚\"which\"ï¼‰ï¼Œå°è¯•æ‰¾åˆ°å‰é¢çš„åè¯ä½œä¸ºå…ˆè¡Œè¯\n",
    "   - å¯¹äºå…¶ä»–ä»£è¯ï¼ˆå¦‚\"it\", \"this\"ï¼‰ï¼Œå¯»æ‰¾å‘½åå®ä½“æˆ–ç›¸å…³åè¯\n",
    "\n",
    "3. **æ™ºèƒ½è¿‡æ»¤é€»è¾‘**ï¼š\n",
    "   - å¦‚æœæ— æ³•æˆåŠŸè§£æä»£è¯ï¼Œç›´æ¥è·³è¿‡è¯¥SVOç»“æ„\n",
    "   - ç¡®ä¿æœ€ç»ˆç»“æœä¸­ä¸åŒ…å«ä»»ä½•ä»£è¯\n",
    "   - ä¿ç•™æœ‰å®è´¨æ„ä¹‰çš„SVOå…³ç³»\n",
    "\n",
    "### é¢„æœŸæ•ˆæœï¼š\n",
    "- ä» `\"which represents wealth\"` æå–åˆ° `\"commodities represent wealth\"`\n",
    "- ä» `\"The paper is expensive which costs 10$\"` æå–åˆ° `\"paper costs 10$\"`\n",
    "- å¤§å¹…å‡å°‘ä»£è¯åœ¨æœ€ç»ˆç»“æœä¸­çš„å‡ºç°\n",
    "\n",
    "### å¤„ç†ç»“æœï¼š\n",
    "- æˆåŠŸå¤„ç†äº† 9,755 ä¸ªå¹²å‡€çš„SVOè§’è‰²å…³ç³»\n",
    "- æ‰€æœ‰ä»£è¯éƒ½ç»è¿‡äº†è¿‡æ»¤æˆ–æŒ‡ä»£æ¶ˆè§£å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2918f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_entities = p.mine_entities(\n",
    "    df['sentence'], \n",
    "    clean_entities = True, \n",
    "    progress_bar = True,\n",
    "    output_path = './output/entities.pkl'\n",
    ")\n",
    "\n",
    "for n in known_entities.most_common(10): print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26666867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from relatio.utils import load_entities\n",
    "known_entities = load_entities('./output/entities.pkl')\n",
    "\n",
    "top_known_entities = [e[0] for e in list(known_entities.most_common(100)) if e[0] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a2fe84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from relatio.narrative_models import NarrativeModel\n",
    "\n",
    "m = NarrativeModel(\n",
    "    clustering = 'kmeans',\n",
    "    PCA = True,\n",
    "    UMAP = True,\n",
    "    roles_considered = ['ARG0', 'B-V', 'B-ARGM-NEG', 'ARG1'],\n",
    "    roles_with_known_entities = ['ARG0','ARG1'],\n",
    "    known_entities = top_known_entities,\n",
    "    assignment_to_known_entities = 'embeddings',\n",
    "    roles_with_unknown_entities = ['ARG0','ARG1'],\n",
    "    threshold = 0.1\n",
    ")    \n",
    "             \n",
    "m.fit(postproc_roles, progress_bar = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0fca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot_selection_metric(metric = 'inertia') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91770832",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot_clusters(path = './output/clusters.pdf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7993e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.clusters_to_txt(path = './output/clusters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1298643",
   "metadata": {},
   "outputs": [],
   "source": [
    "narratives = m.predict(postproc_roles, progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7dbeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from relatio.utils import prettify\n",
    "\n",
    "pretty_narratives = []\n",
    "for n in narratives: \n",
    "    pretty_narratives.append(prettify(n))\n",
    "\n",
    "print(\"=== å…³ç³»ã€å™è¿°å’Œæºæ–‡æœ¬è¿½è¸ªä¿¡æ¯å±•ç¤º ===\")\n",
    "for i in range(10):           \n",
    "    print(f\"\\n--- å…³ç³» {i} ---\")\n",
    "    print(f\"åŸå§‹è§’è‰²: {roles[i]}\")\n",
    "    print(f\"å¤„ç†åè§’è‰²: {postproc_roles[i]}\")\n",
    "    print(f\"ç¾åŒ–å™è¿°: {pretty_narratives[i]}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºæºæ–‡æœ¬è¿½è¸ªä¿¡æ¯\n",
    "    if i in source_mapping:\n",
    "        source_info = source_mapping[i]\n",
    "        print(f\"æºæ–‡æœ¬ID: {source_info['source_text_id']}\")\n",
    "        print(f\"å¥å­ç´¢å¼•: {source_info['sentence_index']}\")\n",
    "        print(f\"åŸå§‹å¥å­: {source_info['original_sentence'][:150]}...\")\n",
    "    else:\n",
    "        print(\"æºæ–‡æœ¬ä¿¡æ¯: æœªæ‰¾åˆ°\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nâœ… æ¯ä¸ªå…³ç³»éƒ½å·²é“¾æ¥åˆ°æºæ–‡æœ¬ï¼Œå‡†å¤‡ç”¨äºRAGåº”ç”¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e38e7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from relatio import build_graph, draw_graph\n",
    "\n",
    "G = build_graph(\n",
    "    narratives, \n",
    "    top_n = 100, \n",
    "    prune_network = True\n",
    ")\n",
    "\n",
    "draw_graph(\n",
    "    G,\n",
    "    notebook = True,\n",
    "    show_buttons = False,\n",
    "    width=\"1600px\",\n",
    "    height=\"1000px\",\n",
    "    output_filename = './output/network_of_narratives.html'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5c2283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºåŒ…å«æºæ–‡æœ¬è¿½è¸ªçš„è¾“å‡ºæ–‡ä»¶ï¼ˆä¸ºRAGåº”ç”¨å‡†å¤‡ï¼‰\n",
    "print(\"=== åˆ›å»ºæºæ–‡æœ¬è¿½è¸ªè¾“å‡ºæ–‡ä»¶ ===\")\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. åˆ›å»ºåŒ…å«æºæ–‡æœ¬è¿½è¸ªçš„è¯¦ç»†æ•°æ®ç»“æ„\n",
    "detailed_relations = []\n",
    "\n",
    "for i, (narrative, pretty_narrative) in enumerate(zip(narratives, pretty_narratives)):\n",
    "    relation_data = {\n",
    "        'relation_id': i,\n",
    "        'svo_relation': {\n",
    "            'subject': roles[i]['subject'] if i < len(roles) else '',\n",
    "            'verb': roles[i]['verb'] if i < len(roles) else '',\n",
    "            'object': roles[i]['object'] if i < len(roles) else ''\n",
    "        },\n",
    "        'processed_roles': postproc_roles[i] if i < len(postproc_roles) else {},\n",
    "        'narrative': narrative,\n",
    "        'pretty_narrative': pretty_narrative,\n",
    "        'source_info': source_mapping.get(i, {\n",
    "            'source_text_id': f'unknown_{i}',\n",
    "            'sentence_index': -1,\n",
    "            'original_sentence': '',\n",
    "            'svo_relation': ''\n",
    "        })\n",
    "    }\n",
    "    detailed_relations.append(relation_data)\n",
    "\n",
    "# 2. ä¿å­˜ä¸ºJSONæ–‡ä»¶\n",
    "output_file_json = './output/relations_with_sources.json'\n",
    "with open(output_file_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(detailed_relations, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"è¯¦ç»†å…³ç³»ä¿¡æ¯å·²ä¿å­˜åˆ°: {output_file_json}\")\n",
    "\n",
    "# 3. åˆ›å»ºCSVæ ¼å¼çš„ç®€åŒ–ç‰ˆæœ¬\n",
    "csv_data = []\n",
    "for rel in detailed_relations:\n",
    "    csv_row = {\n",
    "        'relation_id': rel['relation_id'],\n",
    "        'subject': rel['svo_relation']['subject'],\n",
    "        'verb': rel['svo_relation']['verb'],\n",
    "        'object': rel['svo_relation']['object'],\n",
    "        'pretty_narrative': rel['pretty_narrative'],\n",
    "        'source_text_id': rel['source_info']['source_text_id'],\n",
    "        'sentence_index': rel['source_info']['sentence_index'],\n",
    "        'original_sentence': rel['source_info']['original_sentence']\n",
    "    }\n",
    "    csv_data.append(csv_row)\n",
    "\n",
    "relations_df = pd.DataFrame(csv_data)\n",
    "output_file_csv = './output/relations_with_sources.csv'\n",
    "relations_df.to_csv(output_file_csv, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"å…³ç³»CSVæ–‡ä»¶å·²ä¿å­˜åˆ°: {output_file_csv}\")\n",
    "\n",
    "# 4. æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦\n",
    "print(f\"\\n=== è¾“å‡ºæ–‡ä»¶æ‘˜è¦ ===\")\n",
    "print(f\"æ€»å…³ç³»æ•°é‡: {len(detailed_relations)}\")\n",
    "print(f\"æºæ–‡æœ¬æ•°é‡: {len(set([rel['source_info']['source_text_id'] for rel in detailed_relations]))}\")\n",
    "\n",
    "print(f\"\\nç”Ÿæˆçš„æ–‡ä»¶:\")\n",
    "print(f\"- {output_file_json}: å®Œæ•´çš„å…³ç³»å’Œæºæ–‡æœ¬ä¿¡æ¯ (JSONæ ¼å¼)\")\n",
    "print(f\"- {output_file_csv}: å…³ç³»å’Œæºæ–‡æœ¬ä¿¡æ¯è¡¨æ ¼ (CSVæ ¼å¼)\")\n",
    "print(f\"- ./output/source_mapping.json: å…³ç³»ç´¢å¼•åˆ°æºæ–‡æœ¬çš„æ˜ å°„\")\n",
    "\n",
    "print(f\"\\nâœ… æ‰€æœ‰å…³ç³»éƒ½å·²é“¾æ¥åˆ°æºæ–‡æœ¬ï¼Œæ•°æ®å·²å‡†å¤‡å¥½ç”¨äºRAGåº”ç”¨ï¼\")\n",
    "\n",
    "# è®¾ä¸ºå…¨å±€å˜é‡ï¼Œä¾›RAGåº”ç”¨ä½¿ç”¨\n",
    "globals()['detailed_relations'] = detailed_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52053285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æºæ–‡æœ¬è¿½è¸ªæŸ¥è¯¢æ¼”ç¤ºï¼ˆRAGåº”ç”¨å‡†å¤‡ï¼‰\n",
    "print(\"=== æºæ–‡æœ¬è¿½è¸ªæŸ¥è¯¢æ¼”ç¤º ===\")\n",
    "\n",
    "# ç¤ºä¾‹1ï¼šæŸ¥æ‰¾åŒ…å«ç‰¹å®šè¯æ±‡çš„å…³ç³»\n",
    "print(\"\\n1. æœç´¢åŒ…å« 'wealth' çš„å…³ç³»:\")\n",
    "wealth_relations = find_relations_by_keyword('wealth')\n",
    "print(f\"æ‰¾åˆ° {len(wealth_relations)} ä¸ªç›¸å…³å…³ç³»\")\n",
    "\n",
    "for i, rel in enumerate(wealth_relations[:3]):  # æ˜¾ç¤ºå‰3ä¸ª\n",
    "    print(f\"\\nå…³ç³» {i+1}:\")\n",
    "    print(f\"  SVO: {rel['relation']}\")\n",
    "    print(f\"  æºæ–‡æœ¬ID: {rel['source_text_id']}\")\n",
    "    print(f\"  åŸå¥: {rel['original_sentence'][:100]}...\")\n",
    "\n",
    "# ç¤ºä¾‹2ï¼šåˆ†ææºæ–‡æœ¬åˆ†å¸ƒ\n",
    "print(f\"\\n2. æºæ–‡æœ¬åˆ†å¸ƒåˆ†æ:\")\n",
    "source_counts = {}\n",
    "for rel in detailed_relations:\n",
    "    source_id = rel['source_info']['source_text_id']\n",
    "    source_counts[source_id] = source_counts.get(source_id, 0) + 1\n",
    "\n",
    "top_sources = sorted(source_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\"å…³ç³»æ•°é‡æœ€å¤šçš„å‰5ä¸ªæºæ–‡æœ¬:\")\n",
    "for source_id, count in top_sources:\n",
    "    print(f\"  {source_id}: {count} ä¸ªå…³ç³»\")\n",
    "\n",
    "# ç¤ºä¾‹3ï¼šæ ¹æ®æºæ–‡æœ¬IDæŸ¥æ‰¾å…³ç³»\n",
    "if top_sources:\n",
    "    top_source_id = top_sources[0][0]\n",
    "    print(f\"\\n3. æºæ–‡æœ¬ '{top_source_id}' çš„å…³ç³»:\")\n",
    "    source_relations = find_relations_by_source_id(top_source_id)\n",
    "    \n",
    "    for i, rel in enumerate(source_relations[:3]):  # æ˜¾ç¤ºå‰3ä¸ª\n",
    "        print(f\"  {i+1}. {rel['relation']}\")\n",
    "\n",
    "print(f\"\\nâœ… æºæ–‡æœ¬è¿½è¸ªåŠŸèƒ½éªŒè¯å®Œæˆï¼\")\n",
    "print(\"ç°åœ¨ä½ å¯ä»¥:\")\n",
    "print(\"âœ“ æ ¹æ®å…³ç³»IDè¿½è¸ªåˆ°åŸå§‹æ–‡æœ¬\")\n",
    "print(\"âœ“ æ ¹æ®æºæ–‡æœ¬IDæ‰¾åˆ°æ‰€æœ‰ç›¸å…³å…³ç³»\") \n",
    "print(\"âœ“ æ ¹æ®å…³é”®è¯æœç´¢å…³ç³»å¹¶è·å–æºæ–‡æœ¬\")\n",
    "print(\"âœ“ ä¸ºRAGåº”ç”¨æä¾›å®Œæ•´çš„æ–‡æœ¬è¿½è¸ªæ”¯æŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd98ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== æµ‹è¯•Spark APIé…ç½®ï¼ˆä¿®æ­£HMACè®¤è¯ï¼‰===\n",
      "API URL: https://spark-api-open.xf-yun.com/v2/chat/completions\n",
      "ç­¾åå­—ç¬¦ä¸²: 'request-line: POST /v2/chat/completions HTTP/1.1\\ndate: Fri, 27 Jun 2025 11:13:57 GMT\\nhost: spark-api-open.xf-yun.com'\n",
      "ç”Ÿæˆçš„è®¤è¯å¤´: {'Date': 'Fri, 27 Jun 2025 11:13:57 GMT', 'Authorization': 'api_key=\"24714653c4520497d852b63887c4c2f6\", algorithm=\"hmac-sha256\", headers=\"request-line date host\", signature=\"WRSX5tj3StUktePnqdcP9VAzJs331zcqTtm89WLmJOI=\"', 'Host': 'spark-api-open.xf-yun.com', 'Content-Type': 'application/json'}\n",
      "æ­£åœ¨å‘é€æµ‹è¯•è¯·æ±‚...\n",
      "å“åº”çŠ¶æ€ç : 401\n",
      "å“åº”å¤´: {'Date': 'Fri, 27 Jun 2025 11:14:20 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Connection': 'keep-alive', 'Content-Length': '43', 'Server': 'kong/1.3.0'}\n",
      "APIè¯·æ±‚å¤±è´¥: 401\n",
      "é”™è¯¯ä¿¡æ¯: {\"message\":\"HMAC signature does not match\"}\n",
      "\n",
      "âœ… APIæµ‹è¯•å¤±è´¥\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•Spark APIé…ç½®ï¼ˆä½¿ç”¨ç›´æ¥è®¤è¯tokenï¼‰\n",
    "import os\n",
    "import requests\n",
    "import hmac\n",
    "import hashlib\n",
    "import base64\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "def test_spark_api_with_token():\n",
    "    \"\"\"ä½¿ç”¨ç›´æ¥è®¤è¯tokenæµ‹è¯•Spark API\"\"\"\n",
    "    print(\"=== æµ‹è¯•Spark APIé…ç½®ï¼ˆä½¿ç”¨è®¤è¯tokenï¼‰===\")\n",
    "    \n",
    "    # ä½¿ç”¨ç”¨æˆ·æä¾›çš„è®¤è¯ä¿¡æ¯\n",
    "    api_url = 'https://spark-api-open.xf-yun.com/v2/chat/completions'\n",
    "    auth_token = 'iSC****DSl'  # ç”¨æˆ·æä¾›çš„ç³»ç»Ÿé»˜è®¤token\n",
    "    \n",
    "    print(f\"API URL: {api_url}\")\n",
    "    print(f\"ä½¿ç”¨è®¤è¯token: {auth_token}\")\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {auth_token}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"generalv3.5\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hello, please respond with 'API test successful' if you receive this message.\"\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 0.1,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(\"æ­£åœ¨å‘é€æµ‹è¯•è¯·æ±‚...\")\n",
    "        response = requests.post(api_url, headers=headers, json=payload, timeout=30)\n",
    "        \n",
    "        print(f\"å“åº”çŠ¶æ€ç : {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(\"ğŸ‰ APIæµ‹è¯•æˆåŠŸ!\")\n",
    "            print(f\"å“åº”å†…å®¹: {result}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"APIè¯·æ±‚å¤±è´¥: {response.status_code}\")\n",
    "            print(f\"é”™è¯¯ä¿¡æ¯: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"APIè°ƒç”¨å¼‚å¸¸: {e}\")\n",
    "        return False\n",
    "\n",
    "def test_spark_api_websocket_style():\n",
    "    \"\"\"å°è¯•ä½¿ç”¨WebSocketé£æ ¼çš„è®¤è¯\"\"\"\n",
    "    print(\"\\n=== æµ‹è¯•WebSocketé£æ ¼è®¤è¯ ===\")\n",
    "    \n",
    "    api_url = 'https://spark-api-open.xf-yun.com/v2/chat/completions'\n",
    "    app_id = os.getenv('SPARK_APP_ID', '5557b9da')\n",
    "    api_key = os.getenv('SPARK_API_KEY', '24714653c4520497d852b63887c4c2f6')\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_key}',\n",
    "        'X-App-Id': app_id,\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"generalv3.5\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"è¯·å›å¤'æµ‹è¯•æˆåŠŸ'å¦‚æœä½ æ”¶åˆ°è¿™æ¡æ¶ˆæ¯ã€‚\"\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(\"å‘é€WebSocketé£æ ¼è®¤è¯æµ‹è¯•...\")\n",
    "        response = requests.post(api_url, headers=headers, json=payload, timeout=30)\n",
    "        \n",
    "        print(f\"å“åº”çŠ¶æ€ç : {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(\"ğŸ‰ WebSocketé£æ ¼è®¤è¯æˆåŠŸ!\")\n",
    "            print(f\"å“åº”å†…å®¹: {result}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"è¯·æ±‚å¤±è´¥: {response.status_code}\")\n",
    "            print(f\"é”™è¯¯ä¿¡æ¯: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"å¼‚å¸¸: {e}\")\n",
    "        return False\n",
    "\n",
    "# æµ‹è¯•ä¸¤ç§æ–¹å¼\n",
    "print(\"å¼€å§‹æµ‹è¯•ä¸åŒçš„è®¤è¯æ–¹å¼...\")\n",
    "\n",
    "# æ–¹å¼1ï¼šç›´æ¥token\n",
    "token_result = test_spark_api_with_token()\n",
    "\n",
    "# æ–¹å¼2ï¼šWebSocketé£æ ¼\n",
    "ws_result = test_spark_api_websocket_style()\n",
    "\n",
    "if token_result or ws_result:\n",
    "    print(\"\\nâœ… è‡³å°‘æœ‰ä¸€ç§è®¤è¯æ–¹å¼æˆåŠŸï¼\")\n",
    "    successful_method = \"Token\" if token_result else \"WebSocket Style\"\n",
    "    print(f\"æˆåŠŸçš„è®¤è¯æ–¹å¼: {successful_method}\")\n",
    "else:\n",
    "    print(\"\\nâŒ æ‰€æœ‰è®¤è¯æ–¹å¼éƒ½å¤±è´¥äº†\")\n",
    "    print(\"å¯èƒ½éœ€è¦æ£€æŸ¥APIå‡­æ®æˆ–ä½¿ç”¨å…¶ä»–è®¤è¯æ–¹æ³•\")\n",
    "\n",
    "# æ˜¾ç¤ºå½“å‰ç¯å¢ƒå˜é‡ï¼ˆç”¨äºè°ƒè¯•ï¼‰\n",
    "print(f\"\\nğŸ” å½“å‰ç¯å¢ƒå˜é‡:\")\n",
    "print(f\"SPARK_APP_ID: {os.getenv('SPARK_APP_ID', 'æœªè®¾ç½®')}\")\n",
    "print(f\"SPARK_API_KEY: {os.getenv('SPARK_API_KEY', 'æœªè®¾ç½®')[:10]}...\")\n",
    "print(f\"SPARK_API_SECRET: {os.getenv('SPARK_API_SECRET', 'æœªè®¾ç½®')[:10]}...\")\n",
    "print(f\"SPARK_HTTP_URL: {os.getenv('SPARK_HTTP_URL', 'æœªè®¾ç½®')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relatio_env_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
