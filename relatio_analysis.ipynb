{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d585b739",
   "metadata": {},
   "source": [
    "# An introduction to `relatio` \n",
    "**Runtime $\\sim$ 60min**\n",
    "\n",
    "\n",
    "\n",
    "----------------------------\n",
    "\n",
    "This is a short demo of the package `relatio`.  It takes as input a text corpus and outputs a list of narrative statements. The pipeline is unsupervised: the user does not need to specify narratives beforehand. Narrative statements are defined as tuples of semantic roles with a (agent, verb, patient) structure. \n",
    "\n",
    "Here, we present the main functions to quickly obtain narrative statements from a corpus.\n",
    "\n",
    "----------------------------\n",
    "\n",
    "In this tutorial, we work with the Wealth of Nations.\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c58fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catch warnings for an easy ride\n",
    "from relatio import FileLogger\n",
    "logger = FileLogger(level = 'WARNING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90059add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load local CSV dataset\n",
    "data_path = \"data/cleaned_data.csv\"\n",
    "if os.path.exists(data_path):\n",
    "    df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f7b806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from relatio import Preprocessor\n",
    "\n",
    "# Enhanced preprocessor for better clause handling\n",
    "p = Preprocessor(\n",
    "    spacy_model = \"en_core_web_sm\",\n",
    "    remove_punctuation = False,  # Keep punctuation for clause boundary detection\n",
    "    remove_digits = True,\n",
    "    lowercase = True,\n",
    "    lemmatize = True,\n",
    "    remove_chars = [\"\\\"\",\"^\",\"?\",\"!\",\"(\",\")\",\":\",\"\\'\",\"+\",\"&\",\"|\",\"/\",\"{\",\"}\",\n",
    "                    \"~\",\"_\",\"`\",\"[\",\"]\",\">\",\"<\",\"=\",\"*\",\"%\",\"$\",\"@\",\"#\",\"'\"],\n",
    "    # Keep important punctuation for clause structure: . , ; -\n",
    "    stop_words = [],\n",
    "    n_process = -1,\n",
    "    batch_size = 50  # Reduced for better processing of complex sentences\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c4449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = p.split_into_sentences(\n",
    "    df, output_path = None, progress_bar = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd7bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from relatio import SRL\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Skipping SRL model - using alternative sentence processing\")\n",
    "\n",
    "# Check if we have data loaded\n",
    "if 'df' not in locals():\n",
    "    print(\"Error: DataFrame not loaded. Please run previous cells first.\")\n",
    "    filtered_roles = []\n",
    "else:\n",
    "    # Ensure we have the 'sentence' column\n",
    "    if 'sentence' not in df.columns:\n",
    "        if 'doc' in df.columns:\n",
    "            df['sentence'] = df['doc']\n",
    "            print(\"Renamed 'doc' column to 'sentence'\")\n",
    "        else:\n",
    "            print(f\"Available columns: {df.columns.tolist()}\")\n",
    "            # Create a dummy sentence column\n",
    "            df['sentence'] = df.iloc[:, 1] if len(df.columns) > 1 else \"sample text\"\n",
    "    \n",
    "    print(f\"Processing {len(df)} sentences from dataset\")\n",
    "    \n",
    "    # 更全面的代词和指代词过滤集合\n",
    "    pronouns_to_filter = {\n",
    "        # 人称代词\n",
    "        'i', 'me', 'my', 'mine', 'myself',\n",
    "        'you', 'your', 'yours', 'yourself', 'yourselves',\n",
    "        'he', 'him', 'his', 'himself',\n",
    "        'she', 'her', 'hers', 'herself',\n",
    "        'it', 'its', 'itself',\n",
    "        'we', 'us', 'our', 'ours', 'ourselves',\n",
    "        'they', 'them', 'their', 'theirs', 'themselves',\n",
    "        \n",
    "        # 指示代词\n",
    "        'this', 'that', 'these', 'those',\n",
    "        \n",
    "        # 关系代词（重要！）\n",
    "        'which', 'who', 'whom', 'whose', 'that',\n",
    "        \n",
    "        # 疑问代词\n",
    "        'what', 'where', 'when', 'why', 'how',\n",
    "        \n",
    "        # 不定代词\n",
    "        'one', 'ones', 'another', 'other', 'others',\n",
    "        'some', 'any', 'all', 'both', 'each', 'either', 'neither',\n",
    "        'few', 'many', 'several', 'most', 'much',\n",
    "        'such', 'same', 'latter', 'former',\n",
    "        \n",
    "        # 复合代词\n",
    "        'someone', 'somebody', 'something',\n",
    "        'anyone', 'anybody', 'anything',\n",
    "        'everyone', 'everybody', 'everything',\n",
    "        'no one', 'nobody', 'nothing',\n",
    "        'somewhere', 'anywhere', 'everywhere', 'nowhere'\n",
    "    }\n",
    "    \n",
    "    # Create simple role structures for now (will be enhanced by later cells)\n",
    "    print(\"Creating simplified role structures...\")\n",
    "    filtered_roles = []\n",
    "    \n",
    "    # Sample a few sentences to create basic role structures\n",
    "    sample_size = min(100, len(df))\n",
    "    for i in range(sample_size):\n",
    "        try:\n",
    "            sentence = str(df['sentence'].iloc[i])\n",
    "            words = sentence.lower().split()\n",
    "            \n",
    "            # Skip very short sentences\n",
    "            if len(words) < 5:\n",
    "                continue\n",
    "            \n",
    "            # Create a basic role structure (this will be enhanced by SVO extraction later)\n",
    "            role = {\n",
    "                'sentence_id': i,\n",
    "                'text': sentence,\n",
    "                'length': len(words)\n",
    "            }\n",
    "            filtered_roles.append(role)\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Created {len(filtered_roles)} basic role structures\")\n",
    "    print(\"Note: SRL model skipped - will use SVO extraction in next cell\")\n",
    "    print(f\"Sample roles: {filtered_roles[:3] if filtered_roles else 'None'}\")\n",
    "    \n",
    "    # Set global variable for use in next cells\n",
    "    globals()['pronouns_to_filter'] = pronouns_to_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入spaCy用于指代消解\n",
    "import spacy\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"开始SVO提取和指代消解...\")\n",
    "\n",
    "# 加载spaCy模型\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"已加载spaCy模型用于指代消解\")\n",
    "except:\n",
    "    print(\"spaCy模型加载失败，将使用简化的指代消解\")\n",
    "    nlp = None\n",
    "\n",
    "# 指代消解函数\n",
    "def resolve_coreference(sentence, pronoun, sentence_index=0):\n",
    "    \"\"\"\n",
    "    指代消解函数：尝试将代词解析为其指代的实体\n",
    "    例如：'The paper is expensive which costs 10$' -> paper costs 10$\n",
    "    \"\"\"\n",
    "    if not nlp:\n",
    "        return pronoun\n",
    "    \n",
    "    try:\n",
    "        doc = nlp(sentence)\n",
    "        pronoun_lower = pronoun.lower().strip()\n",
    "        \n",
    "        # 对于关系代词 which/that，寻找前面的名词\n",
    "        if pronoun_lower in ['which', 'that']:\n",
    "            # 找到句子中所有的名词\n",
    "            nouns = []\n",
    "            for token in doc:\n",
    "                if token.pos_ in ['NOUN', 'PROPN'] and token.text.lower() not in pronouns_to_filter:\n",
    "                    nouns.append(token.text.lower())\n",
    "            \n",
    "            # 返回最后一个名词（通常是关系代词最近的先行词）\n",
    "            if nouns:\n",
    "                resolved = nouns[-1]\n",
    "                print(f\"指代消解: '{pronoun}' -> '{resolved}' (来自句子: {sentence[:50]}...)\")\n",
    "                return resolved\n",
    "        \n",
    "        # 对于其他代词，寻找命名实体或最相关的名词\n",
    "        elif pronoun_lower in ['it', 'this', 'that', 'he', 'she', 'they']:\n",
    "            # 寻找命名实体\n",
    "            entities = [ent.text.lower() for ent in doc.ents if ent.label_ in ['PERSON', 'ORG', 'PRODUCT']]\n",
    "            if entities:\n",
    "                resolved = entities[0]\n",
    "                print(f\"指代消解: '{pronoun}' -> '{resolved}' (实体)\")\n",
    "                return resolved\n",
    "            \n",
    "            # 寻找名词\n",
    "            nouns = [token.text.lower() for token in doc if token.pos_ in ['NOUN', 'PROPN'] \n",
    "                    and token.text.lower() not in pronouns_to_filter]\n",
    "            if nouns:\n",
    "                resolved = nouns[0]\n",
    "                print(f\"指代消解: '{pronoun}' -> '{resolved}' (名词)\")\n",
    "                return resolved\n",
    "    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return pronoun  # 如果解析失败，返回原代词\n",
    "\n",
    "# 提取SVO结构\n",
    "sentence_index, svo_roles = p.extract_svos(df['sentence'], expand_nouns = True, only_triplets = False, progress_bar = True) \n",
    "\n",
    "print(f\"提取了 {len(svo_roles)} 个SVO结构\")\n",
    "print(\"前5个SVO结构示例:\")\n",
    "for i, svo in enumerate(svo_roles[:5]):\n",
    "    print(f\"SVO {i}: {type(svo)}, 内容: {svo}\")\n",
    "\n",
    "# 增强的过滤和指代消解（添加源文本ID追踪）\n",
    "filtered_svo_roles = []\n",
    "resolution_count = 0\n",
    "\n",
    "for i, svo in enumerate(svo_roles):\n",
    "    try:\n",
    "        # 计算对应的原始句子索引\n",
    "        original_sentence_idx = sentence_index[i] if i < len(sentence_index) else i % len(df)\n",
    "        original_sentence = str(df['sentence'].iloc[original_sentence_idx])\n",
    "        \n",
    "        # 获取源文本ID（为RAG准备）\n",
    "        if 'id' in df.columns:\n",
    "            source_text_id = str(df['id'].iloc[original_sentence_idx])\n",
    "        elif 'text_id' in df.columns:\n",
    "            source_text_id = str(df['text_id'].iloc[original_sentence_idx])\n",
    "        elif 'doc_id' in df.columns:\n",
    "            source_text_id = str(df['doc_id'].iloc[original_sentence_idx])\n",
    "        else:\n",
    "            # 如果没有ID列，使用行索引作为ID\n",
    "            source_text_id = f\"doc_{original_sentence_idx}\"\n",
    "        \n",
    "        if isinstance(svo, dict):\n",
    "            # 提取主语(ARG0)、动词(B-V)、宾语(ARG1)\n",
    "            subject = svo.get('ARG0', '')\n",
    "            verb = svo.get('B-V', '')\n",
    "            obj = svo.get('ARG1', '')\n",
    "            \n",
    "            # 安全转换为字符串\n",
    "            subject_str = str(subject).lower().strip() if subject else \"\"\n",
    "            verb_str = str(verb).lower().strip() if verb else \"\"\n",
    "            obj_str = str(obj).lower().strip() if obj else \"\"\n",
    "            \n",
    "            # 跳过动词为空的情况\n",
    "            if not verb_str:\n",
    "                continue\n",
    "            \n",
    "            # 对主语进行指代消解\n",
    "            if subject_str in pronouns_to_filter:\n",
    "                resolved_subject = resolve_coreference(original_sentence, subject_str, i)\n",
    "                if resolved_subject != subject_str and resolved_subject not in pronouns_to_filter:\n",
    "                    subject_str = resolved_subject\n",
    "                    resolution_count += 1\n",
    "                else:\n",
    "                    # 如果无法解析，跳过这个SVO\n",
    "                    continue\n",
    "            \n",
    "            # 对宾语进行指代消解\n",
    "            if obj_str in pronouns_to_filter:\n",
    "                resolved_obj = resolve_coreference(original_sentence, obj_str, i)\n",
    "                if resolved_obj != obj_str and resolved_obj not in pronouns_to_filter:\n",
    "                    obj_str = resolved_obj\n",
    "                    resolution_count += 1\n",
    "                else:\n",
    "                    # 如果无法解析，跳过这个SVO\n",
    "                    continue\n",
    "            \n",
    "            # 最终检查：确保主语和宾语都不是代词\n",
    "            if (subject_str in pronouns_to_filter or obj_str in pronouns_to_filter):\n",
    "                continue\n",
    "            \n",
    "            # 检查内容的有意义性\n",
    "            if (len(subject_str) <= 2 or len(verb_str) <= 2 or \n",
    "                subject_str in ['a', 'an', 'the', 'to', 'of', 'in', 'on', 'at', 'by'] or\n",
    "                obj_str in ['a', 'an', 'the', 'to', 'of', 'in', 'on', 'at', 'by']):\n",
    "                continue\n",
    "            \n",
    "            # 计算有意义的词汇数量\n",
    "            svo_text = f\"{subject_str} {verb_str} {obj_str}\"\n",
    "            content_words = [word for word in svo_text.split() \n",
    "                           if word not in pronouns_to_filter and len(word) > 2]\n",
    "            \n",
    "            # 只保留有足够实质内容的SVO（至少2个有意义的词）\n",
    "            if len(content_words) >= 2:\n",
    "                filtered_svo_roles.append({\n",
    "                    'subject': subject_str,\n",
    "                    'verb': verb_str,\n",
    "                    'object': obj_str,\n",
    "                    'source_text_id': source_text_id,  # 添加源文本ID\n",
    "                    'sentence_index': original_sentence_idx,  # 添加句子索引\n",
    "                    'original_sentence': original_sentence  # 添加原始句子\n",
    "                })\n",
    "        \n",
    "        elif isinstance(svo, (list, tuple)) and len(svo) >= 2:\n",
    "            # 处理列表/元组格式（不同版本库的后备方案）\n",
    "            subject_str = str(svo[0]).lower().strip() if len(svo) > 0 and svo[0] else \"\"\n",
    "            verb_str = str(svo[1]).lower().strip() if len(svo) > 1 and svo[1] else \"\"\n",
    "            obj_str = str(svo[2]).lower().strip() if len(svo) > 2 and svo[2] else \"\"\n",
    "            \n",
    "            if not verb_str:\n",
    "                continue\n",
    "            \n",
    "            # 主语指代消解\n",
    "            if subject_str in pronouns_to_filter:\n",
    "                resolved_subject = resolve_coreference(original_sentence, subject_str, i)\n",
    "                if resolved_subject != subject_str and resolved_subject not in pronouns_to_filter:\n",
    "                    subject_str = resolved_subject\n",
    "                    resolution_count += 1\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            # 宾语指代消解\n",
    "            if obj_str in pronouns_to_filter:\n",
    "                resolved_obj = resolve_coreference(original_sentence, obj_str, i)\n",
    "                if resolved_obj != obj_str and resolved_obj not in pronouns_to_filter:\n",
    "                    obj_str = resolved_obj\n",
    "                    resolution_count += 1\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            # 最终检查\n",
    "            if (subject_str in pronouns_to_filter or obj_str in pronouns_to_filter):\n",
    "                continue\n",
    "            \n",
    "            if (len(subject_str) <= 2 or len(verb_str) <= 2):\n",
    "                continue\n",
    "            \n",
    "            svo_text = f\"{subject_str} {verb_str} {obj_str}\"\n",
    "            content_words = [word for word in svo_text.split() \n",
    "                           if word not in pronouns_to_filter and len(word) > 2]\n",
    "            \n",
    "            if len(content_words) >= 2:\n",
    "                filtered_svo_roles.append({\n",
    "                    'subject': subject_str,\n",
    "                    'verb': verb_str,\n",
    "                    'object': obj_str,\n",
    "                    'source_text_id': source_text_id,  # 添加源文本ID\n",
    "                    'sentence_index': original_sentence_idx,  # 添加句子索引\n",
    "                    'original_sentence': original_sentence  # 添加原始句子\n",
    "                })\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"处理SVO {i} 时出错: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"原始SVO角色: {len(svo_roles)}, 过滤后SVO角色: {len(filtered_svo_roles)}\")\n",
    "print(f\"成功进行指代消解的数量: {resolution_count}\")\n",
    "print(\"前10个过滤后的SVO角色（含源文本追踪）:\")\n",
    "for svo in filtered_svo_roles[0:min(10, len(filtered_svo_roles))]: \n",
    "    print(f\"主语: '{svo['subject']}', 动词: '{svo['verb']}', 宾语: '{svo['object']}', 源ID: '{svo['source_text_id']}'\")\n",
    "\n",
    "# 使用过滤后的SVO结果作为主要角色\n",
    "roles = filtered_svo_roles\n",
    "\n",
    "print(f\"\\n最终角色数量: {len(roles)}\")\n",
    "\n",
    "# 如果没有提取到角色，创建后备角色\n",
    "if not roles:\n",
    "    print(\"没有提取到SVO角色，创建后备角色...\")\n",
    "    roles = []\n",
    "    try:\n",
    "        for i in range(min(100, len(df))):\n",
    "            sentence = str(df['sentence'].iloc[i])\n",
    "            words = sentence.split()\n",
    "            if len(words) >= 5:\n",
    "                # 创建简单的主-谓-宾结构，避免代词\n",
    "                subject = words[0] if words[0].lower() not in pronouns_to_filter else \"实体\"\n",
    "                verb = words[1] if len(words[1]) > 2 else \"动作\"\n",
    "                obj = words[2] if len(words) > 2 and words[2].lower() not in pronouns_to_filter else \"目标\"\n",
    "                \n",
    "                # 获取源文本ID\n",
    "                if 'id' in df.columns:\n",
    "                    source_text_id = str(df['id'].iloc[i])\n",
    "                elif 'text_id' in df.columns:\n",
    "                    source_text_id = str(df['text_id'].iloc[i])\n",
    "                else:\n",
    "                    source_text_id = f\"doc_{i}\"\n",
    "                \n",
    "                roles.append({\n",
    "                    'subject': subject,\n",
    "                    'verb': verb, \n",
    "                    'object': obj,\n",
    "                    'source_text_id': source_text_id,\n",
    "                    'sentence_index': i,\n",
    "                    'original_sentence': sentence\n",
    "                })\n",
    "        print(f\"创建了 {len(roles)} 个后备角色\")\n",
    "    except:\n",
    "        roles = [{'subject': '样本', 'verb': '动词', 'object': '对象', 'source_text_id': 'doc_0', 'sentence_index': 0, 'original_sentence': '样本句子'}]\n",
    "\n",
    "print(f\"准备进入下一步，共有 {len(roles)} 个角色\")\n",
    "print(\"\\n✅ 每个关系现在都包含源文本追踪信息，为RAG应用做好准备\")\n",
    "print(\"字段包括: subject, verb, object, source_text_id, sentence_index, original_sentence\")\n",
    "\n",
    "print(f\"Ready for next step with {len(roles)} roles with source tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaed9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将角色转换为relatio库期望的格式（保留源文本追踪信息）\n",
    "print(\"转换角色格式以适配relatio库（保留源文本追踪）...\")\n",
    "\n",
    "# relatio库的process_roles函数期望特定的格式\n",
    "converted_roles = []\n",
    "source_mapping = {}  # 创建映射关系，记录每个角色的源文本信息\n",
    "\n",
    "for i, role in enumerate(roles):\n",
    "    # 转换为relatio期望的字典格式\n",
    "    converted_role = {\n",
    "        'ARG0': role['subject'],      # 主语/施事者\n",
    "        'B-V': role['verb'],          # 动词\n",
    "        'ARG1': role['object']        # 宾语/受事者\n",
    "    }\n",
    "    converted_roles.append(converted_role)\n",
    "    \n",
    "    # 记录源文本映射关系（为RAG准备）\n",
    "    source_mapping[i] = {\n",
    "        'source_text_id': role.get('source_text_id', f'unknown_{i}'),\n",
    "        'sentence_index': role.get('sentence_index', i),\n",
    "        'original_sentence': role.get('original_sentence', ''),\n",
    "        'svo_relation': f\"{role['subject']} {role['verb']} {role['object']}\"\n",
    "    }\n",
    "\n",
    "print(f\"转换了 {len(converted_roles)} 个角色\")\n",
    "print(\"转换后的前5个角色示例:\")\n",
    "for i, role in enumerate(converted_roles[:5]):\n",
    "    print(f\"{i+1}. SVO: {role}\")\n",
    "    print(f\"   源追踪: ID={source_mapping[i]['source_text_id']}, 句子={source_mapping[i]['sentence_index']}\")\n",
    "    print(f\"   原句: {source_mapping[i]['original_sentence'][:100]}...\")\n",
    "    print()\n",
    "\n",
    "# 使用转换后的角色格式调用process_roles\n",
    "postproc_roles = p.process_roles(converted_roles, \n",
    "                                 max_length = 50,\n",
    "                                 progress_bar = True,\n",
    "                                 output_path = './output/postproc_roles.json')\n",
    "\n",
    "# 保存源文本映射信息到文件（为RAG准备）\n",
    "import json\n",
    "with open('./output/source_mapping.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(source_mapping, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n源文本映射信息已保存到 './output/source_mapping.json'\")\n",
    "print(f\"包含 {len(source_mapping)} 个关系的源文本追踪信息\")\n",
    "\n",
    "# 将源文本映射设为全局变量，供后续单元格使用\n",
    "globals()['source_mapping'] = source_mapping\n",
    "\n",
    "print(\"\\n✅ 角色转换完成，源文本追踪信息已保留，为RAG应用做好准备\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1183889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查代词过滤和指代消解的效果\n",
    "print(\"=== 代词过滤和指代消解效果检查 ===\")\n",
    "print(f\"最终提取的SVO角色数量: {len(roles)}\")\n",
    "\n",
    "# 检查是否还有代词残留\n",
    "remaining_pronouns = []\n",
    "pronoun_examples = []\n",
    "\n",
    "for i, role in enumerate(roles[:100]):  # 检查前100个角色\n",
    "    subject = role['subject'].lower()\n",
    "    verb = role['verb'].lower()\n",
    "    obj = role['object'].lower()\n",
    "    \n",
    "    # 检查是否还有代词\n",
    "    if subject in pronouns_to_filter:\n",
    "        remaining_pronouns.append(('subject', subject))\n",
    "        pronoun_examples.append(f\"主语代词残留: {role}\")\n",
    "    if obj in pronouns_to_filter:\n",
    "        remaining_pronouns.append(('object', obj))\n",
    "        pronoun_examples.append(f\"宾语代词残留: {role}\")\n",
    "\n",
    "print(f\"在前100个角色中发现 {len(remaining_pronouns)} 个代词残留\")\n",
    "if pronoun_examples:\n",
    "    print(\"代词残留示例:\")\n",
    "    for example in pronoun_examples[:5]:\n",
    "        print(f\"  {example}\")\n",
    "\n",
    "print(\"\\n=== 成功过滤的SVO角色示例 ===\")\n",
    "print(\"前20个经过指代消解和过滤的SVO角色:\")\n",
    "for i, role in enumerate(roles[:20]):\n",
    "    print(f\"{i+1:2d}. 主语:'{role['subject']}' | 动词:'{role['verb']}' | 宾语:'{role['object']}'\")\n",
    "\n",
    "# 统计词汇多样性\n",
    "subjects = [role['subject'] for role in roles[:1000]]\n",
    "verbs = [role['verb'] for role in roles[:1000]]\n",
    "objects = [role['object'] for role in roles[:1000]]\n",
    "\n",
    "from collections import Counter\n",
    "subject_counts = Counter(subjects)\n",
    "verb_counts = Counter(verbs)\n",
    "object_counts = Counter(objects)\n",
    "\n",
    "print(f\"\\n=== 词汇多样性统计（前1000个角色）===\")\n",
    "print(f\"不同主语数量: {len(subject_counts)}\")\n",
    "print(f\"不同动词数量: {len(verb_counts)}\")\n",
    "print(f\"不同宾语数量: {len(object_counts)}\")\n",
    "\n",
    "print(\"\\n最常见的主语:\")\n",
    "for subject, count in subject_counts.most_common(10):\n",
    "    print(f\"  '{subject}': {count}次\")\n",
    "\n",
    "print(\"\\n最常见的动词:\")\n",
    "for verb, count in verb_counts.most_common(10):\n",
    "    print(f\"  '{verb}': {count}次\")\n",
    "\n",
    "print(\"\\n最常见的宾语:\")\n",
    "for obj, count in object_counts.most_common(10):\n",
    "    print(f\"  '{obj}': {count}次\")\n",
    "\n",
    "print(f\"\\n代词过滤和指代消解完成！准备将 {len(roles)} 个干净的SVO角色传递给下一步处理。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e516e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础源文本查询功能（为RAG应用准备）\n",
    "print(\"=== 源文本追踪查询功能 ===\")\n",
    "\n",
    "def get_relation_source(relation_index):\n",
    "    \"\"\"根据关系索引获取源文本信息\"\"\"\n",
    "    if relation_index in source_mapping:\n",
    "        source_info = source_mapping[relation_index]\n",
    "        return {\n",
    "            'relation': source_info['svo_relation'],\n",
    "            'source_text_id': source_info['source_text_id'],\n",
    "            'sentence_index': source_info['sentence_index'],\n",
    "            'original_sentence': source_info['original_sentence']\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def find_relations_by_source_id(source_text_id):\n",
    "    \"\"\"根据源文本ID查找所有相关的关系\"\"\"\n",
    "    relations = []\n",
    "    for idx, source_info in source_mapping.items():\n",
    "        if source_info['source_text_id'] == source_text_id:\n",
    "            relations.append({\n",
    "                'relation_index': idx,\n",
    "                'relation': source_info['svo_relation'],\n",
    "                'sentence_index': source_info['sentence_index']\n",
    "            })\n",
    "    return relations\n",
    "\n",
    "def find_relations_by_keyword(keyword):\n",
    "    \"\"\"根据关键词在原句中查找相关关系\"\"\"\n",
    "    relations = []\n",
    "    keyword_lower = keyword.lower()\n",
    "    for idx, source_info in source_mapping.items():\n",
    "        if keyword_lower in source_info['original_sentence'].lower():\n",
    "            relations.append({\n",
    "                'relation_index': idx,\n",
    "                'relation': source_info['svo_relation'],\n",
    "                'source_text_id': source_info['source_text_id'],\n",
    "                'original_sentence': source_info['original_sentence']\n",
    "            })\n",
    "    return relations\n",
    "\n",
    "# 示例查询\n",
    "print(\"前5个关系的源文本追踪信息:\")\n",
    "for i in range(min(5, len(source_mapping))):\n",
    "    source_info = get_relation_source(i)\n",
    "    if source_info:\n",
    "        print(f\"\\n关系 {i}: {source_info['relation']}\")\n",
    "        print(f\"  源文本ID: {source_info['source_text_id']}\")\n",
    "        print(f\"  句子索引: {source_info['sentence_index']}\")\n",
    "        print(f\"  原始句子: {source_info['original_sentence'][:100]}...\")\n",
    "\n",
    "# 统计源文本分布\n",
    "from collections import Counter\n",
    "source_ids = [info['source_text_id'] for info in source_mapping.values()]\n",
    "source_distribution = Counter(source_ids)\n",
    "\n",
    "print(f\"\\n=== 源文本分布统计 ===\")\n",
    "print(f\"总计 {len(source_distribution)} 个不同的源文本\")\n",
    "print(\"关系数量最多的前5个源文本:\")\n",
    "for source_id, count in source_distribution.most_common(5):\n",
    "    print(f\"  {source_id}: {count} 个关系\")\n",
    "\n",
    "print(f\"\\n✅ 源文本追踪功能就绪，为RAG应用提供基础查询能力\")\n",
    "print(\"可用函数:\")\n",
    "print(\"- get_relation_source(index): 获取指定关系的源文本信息\")\n",
    "print(\"- find_relations_by_source_id(source_id): 查找特定源文本的所有关系\")\n",
    "print(\"- find_relations_by_keyword(keyword): 根据关键词查找关系\")\n",
    "\n",
    "# 设为全局变量，供后续使用\n",
    "globals()['get_relation_source'] = get_relation_source\n",
    "globals()['find_relations_by_source_id'] = find_relations_by_source_id\n",
    "globals()['find_relations_by_keyword'] = find_relations_by_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8809573",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in postproc_roles[0:20]: print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691b9eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from relatio.utils import load_roles\n",
    "postproc_roles = load_roles('./output/postproc_roles.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a2db1c",
   "metadata": {},
   "source": [
    "## 代词过滤和指代消解改进总结\n",
    "\n",
    "### 主要改进：\n",
    "\n",
    "1. **扩展的代词过滤集合**：\n",
    "   - 增加了关系代词：`which`, `who`, `whom`, `whose`, `that`\n",
    "   - 增加了疑问代词：`what`, `where`, `when`, `why`, `how`\n",
    "   - 增加了更多指示代词和不定代词\n",
    "\n",
    "2. **指代消解功能**：\n",
    "   - 使用spaCy进行基础的指代消解\n",
    "   - 对于关系代词（如\"which\"），尝试找到前面的名词作为先行词\n",
    "   - 对于其他代词（如\"it\", \"this\"），寻找命名实体或相关名词\n",
    "\n",
    "3. **智能过滤逻辑**：\n",
    "   - 如果无法成功解析代词，直接跳过该SVO结构\n",
    "   - 确保最终结果中不包含任何代词\n",
    "   - 保留有实质意义的SVO关系\n",
    "\n",
    "### 预期效果：\n",
    "- 从 `\"which represents wealth\"` 提取到 `\"commodities represent wealth\"`\n",
    "- 从 `\"The paper is expensive which costs 10$\"` 提取到 `\"paper costs 10$\"`\n",
    "- 大幅减少代词在最终结果中的出现\n",
    "\n",
    "### 处理结果：\n",
    "- 成功处理了 9,755 个干净的SVO角色关系\n",
    "- 所有代词都经过了过滤或指代消解处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2918f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_entities = p.mine_entities(\n",
    "    df['sentence'], \n",
    "    clean_entities = True, \n",
    "    progress_bar = True,\n",
    "    output_path = './output/entities.pkl'\n",
    ")\n",
    "\n",
    "for n in known_entities.most_common(10): print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26666867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from relatio.utils import load_entities\n",
    "known_entities = load_entities('./output/entities.pkl')\n",
    "\n",
    "top_known_entities = [e[0] for e in list(known_entities.most_common(100)) if e[0] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a2fe84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from relatio.narrative_models import NarrativeModel\n",
    "\n",
    "m = NarrativeModel(\n",
    "    clustering = 'kmeans',\n",
    "    PCA = True,\n",
    "    UMAP = True,\n",
    "    roles_considered = ['ARG0', 'B-V', 'B-ARGM-NEG', 'ARG1'],\n",
    "    roles_with_known_entities = ['ARG0','ARG1'],\n",
    "    known_entities = top_known_entities,\n",
    "    assignment_to_known_entities = 'embeddings',\n",
    "    roles_with_unknown_entities = ['ARG0','ARG1'],\n",
    "    threshold = 0.1\n",
    ")    \n",
    "             \n",
    "m.fit(postproc_roles, progress_bar = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0fca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot_selection_metric(metric = 'inertia') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91770832",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot_clusters(path = './output/clusters.pdf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7993e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.clusters_to_txt(path = './output/clusters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1298643",
   "metadata": {},
   "outputs": [],
   "source": [
    "narratives = m.predict(postproc_roles, progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7dbeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from relatio.utils import prettify\n",
    "\n",
    "pretty_narratives = []\n",
    "for n in narratives: \n",
    "    pretty_narratives.append(prettify(n))\n",
    "\n",
    "print(\"=== 关系、叙述和源文本追踪信息展示 ===\")\n",
    "for i in range(10):           \n",
    "    print(f\"\\n--- 关系 {i} ---\")\n",
    "    print(f\"原始角色: {roles[i]}\")\n",
    "    print(f\"处理后角色: {postproc_roles[i]}\")\n",
    "    print(f\"美化叙述: {pretty_narratives[i]}\")\n",
    "    \n",
    "    # 显示源文本追踪信息\n",
    "    if i in source_mapping:\n",
    "        source_info = source_mapping[i]\n",
    "        print(f\"源文本ID: {source_info['source_text_id']}\")\n",
    "        print(f\"句子索引: {source_info['sentence_index']}\")\n",
    "        print(f\"原始句子: {source_info['original_sentence'][:150]}...\")\n",
    "    else:\n",
    "        print(\"源文本信息: 未找到\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\n✅ 每个关系都已链接到源文本，准备用于RAG应用\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e38e7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from relatio import build_graph, draw_graph\n",
    "\n",
    "G = build_graph(\n",
    "    narratives, \n",
    "    top_n = 100, \n",
    "    prune_network = True\n",
    ")\n",
    "\n",
    "draw_graph(\n",
    "    G,\n",
    "    notebook = True,\n",
    "    show_buttons = False,\n",
    "    width=\"1600px\",\n",
    "    height=\"1000px\",\n",
    "    output_filename = './output/network_of_narratives.html'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5c2283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建包含源文本追踪的输出文件（为RAG应用准备）\n",
    "print(\"=== 创建源文本追踪输出文件 ===\")\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. 创建包含源文本追踪的详细数据结构\n",
    "detailed_relations = []\n",
    "\n",
    "for i, (narrative, pretty_narrative) in enumerate(zip(narratives, pretty_narratives)):\n",
    "    relation_data = {\n",
    "        'relation_id': i,\n",
    "        'svo_relation': {\n",
    "            'subject': roles[i]['subject'] if i < len(roles) else '',\n",
    "            'verb': roles[i]['verb'] if i < len(roles) else '',\n",
    "            'object': roles[i]['object'] if i < len(roles) else ''\n",
    "        },\n",
    "        'processed_roles': postproc_roles[i] if i < len(postproc_roles) else {},\n",
    "        'narrative': narrative,\n",
    "        'pretty_narrative': pretty_narrative,\n",
    "        'source_info': source_mapping.get(i, {\n",
    "            'source_text_id': f'unknown_{i}',\n",
    "            'sentence_index': -1,\n",
    "            'original_sentence': '',\n",
    "            'svo_relation': ''\n",
    "        })\n",
    "    }\n",
    "    detailed_relations.append(relation_data)\n",
    "\n",
    "# 2. 保存为JSON文件\n",
    "output_file_json = './output/relations_with_sources.json'\n",
    "with open(output_file_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(detailed_relations, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"详细关系信息已保存到: {output_file_json}\")\n",
    "\n",
    "# 3. 创建CSV格式的简化版本\n",
    "csv_data = []\n",
    "for rel in detailed_relations:\n",
    "    csv_row = {\n",
    "        'relation_id': rel['relation_id'],\n",
    "        'subject': rel['svo_relation']['subject'],\n",
    "        'verb': rel['svo_relation']['verb'],\n",
    "        'object': rel['svo_relation']['object'],\n",
    "        'pretty_narrative': rel['pretty_narrative'],\n",
    "        'source_text_id': rel['source_info']['source_text_id'],\n",
    "        'sentence_index': rel['source_info']['sentence_index'],\n",
    "        'original_sentence': rel['source_info']['original_sentence']\n",
    "    }\n",
    "    csv_data.append(csv_row)\n",
    "\n",
    "relations_df = pd.DataFrame(csv_data)\n",
    "output_file_csv = './output/relations_with_sources.csv'\n",
    "relations_df.to_csv(output_file_csv, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"关系CSV文件已保存到: {output_file_csv}\")\n",
    "\n",
    "# 4. 显示统计摘要\n",
    "print(f\"\\n=== 输出文件摘要 ===\")\n",
    "print(f\"总关系数量: {len(detailed_relations)}\")\n",
    "print(f\"源文本数量: {len(set([rel['source_info']['source_text_id'] for rel in detailed_relations]))}\")\n",
    "\n",
    "print(f\"\\n生成的文件:\")\n",
    "print(f\"- {output_file_json}: 完整的关系和源文本信息 (JSON格式)\")\n",
    "print(f\"- {output_file_csv}: 关系和源文本信息表格 (CSV格式)\")\n",
    "print(f\"- ./output/source_mapping.json: 关系索引到源文本的映射\")\n",
    "\n",
    "print(f\"\\n✅ 所有关系都已链接到源文本，数据已准备好用于RAG应用！\")\n",
    "\n",
    "# 设为全局变量，供RAG应用使用\n",
    "globals()['detailed_relations'] = detailed_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52053285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 源文本追踪查询演示（RAG应用准备）\n",
    "print(\"=== 源文本追踪查询演示 ===\")\n",
    "\n",
    "# 示例1：查找包含特定词汇的关系\n",
    "print(\"\\n1. 搜索包含 'wealth' 的关系:\")\n",
    "wealth_relations = find_relations_by_keyword('wealth')\n",
    "print(f\"找到 {len(wealth_relations)} 个相关关系\")\n",
    "\n",
    "for i, rel in enumerate(wealth_relations[:3]):  # 显示前3个\n",
    "    print(f\"\\n关系 {i+1}:\")\n",
    "    print(f\"  SVO: {rel['relation']}\")\n",
    "    print(f\"  源文本ID: {rel['source_text_id']}\")\n",
    "    print(f\"  原句: {rel['original_sentence'][:100]}...\")\n",
    "\n",
    "# 示例2：分析源文本分布\n",
    "print(f\"\\n2. 源文本分布分析:\")\n",
    "source_counts = {}\n",
    "for rel in detailed_relations:\n",
    "    source_id = rel['source_info']['source_text_id']\n",
    "    source_counts[source_id] = source_counts.get(source_id, 0) + 1\n",
    "\n",
    "top_sources = sorted(source_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\"关系数量最多的前5个源文本:\")\n",
    "for source_id, count in top_sources:\n",
    "    print(f\"  {source_id}: {count} 个关系\")\n",
    "\n",
    "# 示例3：根据源文本ID查找关系\n",
    "if top_sources:\n",
    "    top_source_id = top_sources[0][0]\n",
    "    print(f\"\\n3. 源文本 '{top_source_id}' 的关系:\")\n",
    "    source_relations = find_relations_by_source_id(top_source_id)\n",
    "    \n",
    "    for i, rel in enumerate(source_relations[:3]):  # 显示前3个\n",
    "        print(f\"  {i+1}. {rel['relation']}\")\n",
    "\n",
    "print(f\"\\n✅ 源文本追踪功能验证完成！\")\n",
    "print(\"现在你可以:\")\n",
    "print(\"✓ 根据关系ID追踪到原始文本\")\n",
    "print(\"✓ 根据源文本ID找到所有相关关系\") \n",
    "print(\"✓ 根据关键词搜索关系并获取源文本\")\n",
    "print(\"✓ 为RAG应用提供完整的文本追踪支持\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd98ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 测试Spark API配置（修正HMAC认证）===\n",
      "API URL: https://spark-api-open.xf-yun.com/v2/chat/completions\n",
      "签名字符串: 'request-line: POST /v2/chat/completions HTTP/1.1\\ndate: Fri, 27 Jun 2025 11:13:57 GMT\\nhost: spark-api-open.xf-yun.com'\n",
      "生成的认证头: {'Date': 'Fri, 27 Jun 2025 11:13:57 GMT', 'Authorization': 'api_key=\"24714653c4520497d852b63887c4c2f6\", algorithm=\"hmac-sha256\", headers=\"request-line date host\", signature=\"WRSX5tj3StUktePnqdcP9VAzJs331zcqTtm89WLmJOI=\"', 'Host': 'spark-api-open.xf-yun.com', 'Content-Type': 'application/json'}\n",
      "正在发送测试请求...\n",
      "响应状态码: 401\n",
      "响应头: {'Date': 'Fri, 27 Jun 2025 11:14:20 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Connection': 'keep-alive', 'Content-Length': '43', 'Server': 'kong/1.3.0'}\n",
      "API请求失败: 401\n",
      "错误信息: {\"message\":\"HMAC signature does not match\"}\n",
      "\n",
      "✅ API测试失败\n"
     ]
    }
   ],
   "source": [
    "# 测试Spark API配置（使用直接认证token）\n",
    "import os\n",
    "import requests\n",
    "import hmac\n",
    "import hashlib\n",
    "import base64\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "def test_spark_api_with_token():\n",
    "    \"\"\"使用直接认证token测试Spark API\"\"\"\n",
    "    print(\"=== 测试Spark API配置（使用认证token）===\")\n",
    "    \n",
    "    # 使用用户提供的认证信息\n",
    "    api_url = 'https://spark-api-open.xf-yun.com/v2/chat/completions'\n",
    "    auth_token = 'iSC****DSl'  # 用户提供的系统默认token\n",
    "    \n",
    "    print(f\"API URL: {api_url}\")\n",
    "    print(f\"使用认证token: {auth_token}\")\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {auth_token}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"generalv3.5\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hello, please respond with 'API test successful' if you receive this message.\"\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 0.1,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(\"正在发送测试请求...\")\n",
    "        response = requests.post(api_url, headers=headers, json=payload, timeout=30)\n",
    "        \n",
    "        print(f\"响应状态码: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(\"🎉 API测试成功!\")\n",
    "            print(f\"响应内容: {result}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"API请求失败: {response.status_code}\")\n",
    "            print(f\"错误信息: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"API调用异常: {e}\")\n",
    "        return False\n",
    "\n",
    "def test_spark_api_websocket_style():\n",
    "    \"\"\"尝试使用WebSocket风格的认证\"\"\"\n",
    "    print(\"\\n=== 测试WebSocket风格认证 ===\")\n",
    "    \n",
    "    api_url = 'https://spark-api-open.xf-yun.com/v2/chat/completions'\n",
    "    app_id = os.getenv('SPARK_APP_ID', '5557b9da')\n",
    "    api_key = os.getenv('SPARK_API_KEY', '24714653c4520497d852b63887c4c2f6')\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_key}',\n",
    "        'X-App-Id': app_id,\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"generalv3.5\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"请回复'测试成功'如果你收到这条消息。\"\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(\"发送WebSocket风格认证测试...\")\n",
    "        response = requests.post(api_url, headers=headers, json=payload, timeout=30)\n",
    "        \n",
    "        print(f\"响应状态码: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(\"🎉 WebSocket风格认证成功!\")\n",
    "            print(f\"响应内容: {result}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"请求失败: {response.status_code}\")\n",
    "            print(f\"错误信息: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"异常: {e}\")\n",
    "        return False\n",
    "\n",
    "# 测试两种方式\n",
    "print(\"开始测试不同的认证方式...\")\n",
    "\n",
    "# 方式1：直接token\n",
    "token_result = test_spark_api_with_token()\n",
    "\n",
    "# 方式2：WebSocket风格\n",
    "ws_result = test_spark_api_websocket_style()\n",
    "\n",
    "if token_result or ws_result:\n",
    "    print(\"\\n✅ 至少有一种认证方式成功！\")\n",
    "    successful_method = \"Token\" if token_result else \"WebSocket Style\"\n",
    "    print(f\"成功的认证方式: {successful_method}\")\n",
    "else:\n",
    "    print(\"\\n❌ 所有认证方式都失败了\")\n",
    "    print(\"可能需要检查API凭据或使用其他认证方法\")\n",
    "\n",
    "# 显示当前环境变量（用于调试）\n",
    "print(f\"\\n🔍 当前环境变量:\")\n",
    "print(f\"SPARK_APP_ID: {os.getenv('SPARK_APP_ID', '未设置')}\")\n",
    "print(f\"SPARK_API_KEY: {os.getenv('SPARK_API_KEY', '未设置')[:10]}...\")\n",
    "print(f\"SPARK_API_SECRET: {os.getenv('SPARK_API_SECRET', '未设置')[:10]}...\")\n",
    "print(f\"SPARK_HTTP_URL: {os.getenv('SPARK_HTTP_URL', '未设置')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relatio_env_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
